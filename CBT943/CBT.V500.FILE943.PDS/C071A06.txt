CICS/ESA storage management

IBM's CICS Field Television Network (FTN) broadcast on 14
June, 1991, was dedicated to the topic of storage management,
highlighting the vast differences between CICS/ESA (Version
3) and all prior versions.  In his last public appearance with
IBM, Bob Archambeault provided a level of detail not
available outside of CICS logic manuals - a result of his 30
years with IBM and 20 years with CICS.


Storage Layout

Figure 1 compares the storage layout below the 16-megabyte
line for CICS/MVS and CICS/ESA.  After loading DFHSIP,
CICS/MVS modules are loaded from the top down after
leaving a 200 to 250K buffer for the expansion of the MVS
High Private area.  This buffer is not left if a region size
greater than 16 megabytes is specified (in the EXEC or JOB
statement), which will lead to a failure when the MVS High
Private area attempts to expand.  There is normally no need to
specify a region size greater than 16 megabytes since a default
of 32 megabytes is provided above the line for any region size
specified less than or equal to 16 megabytes.

This top down loading is accomplished by GETMAINing all
available (below the line) memory, loading into its highest
locations, then FREEMAINing the rest.

CICS/ESA, on the other hand, loads DFHSIP above the line
and then allocates any below the line storage required from the
bottom up.  As a result, the storage layout is very different.

DSA has been moved.  Its size is now specified by a
parameter, DSASZE, rather than being what is left over after
everything else has been allocated.  DSASZE replaces region
size as the method of determining the amount of below the line
storage used.  OSCOR, meanwhile, has completely
disappeared.

The new Kernel Stack is also worth noticing.  5.4K is allocated
for each MAXTASK, meaning installations accustomed to
specifying MAXTASK=999 will have a 5.4 megabyte Kernel
Stack.


Storage Management

To make CICS more manageable, the new architecture of
CICS/ESA divides CICS up into domains along functional
lines.  A domain is really a system unto itself.  Any
communication with another domain is through a well defined
interface.

The Storage Management Domain manages all storage above
and below the line.  Previous versions of CICS managed
(suballocated) their own storage below the line, but used
GETMAIN and FREEMAIN directly for each piece of storage
above the line.

Rewritten from scratch, the code for the new Storage Manager
and all of its control blocks reside above the line.  CICS and
user data are kept separate to avoid overruns from one to the
other.  CICS areas and user programs are at the low end of
DSA and EDSA, with user storage at the high end.  This
approach eliminates the possibility of table and array indexes
and subscripts exceeding their bounds and corrupting programs
and CICS areas.

All CICS control blocks have been protected by moving them
out of the user storage area and into the CICS area.  In fact,
some have been moved out of DSA completely, for example:

o      Storage manager control blocks

o      Free Area Queue Elements (FAQEs)

o      Free Storage Descriptors

o      Element Descriptors

Previously, a program that accidentally overlaid a FAQE would
bring down the entire CICS region the next time the FAQE was
referenced.

Further protection is provided with the addition of 'crumple
zones' to the front and back of all allocated user storage.  CICS
checks these areas for any change to their pre-defined bit
pattern, which would indicate an overflow of the user storage.

Short-on-Storage (SOS) and stall handling have both been
improved, and a new program compression algorithm
implemented.  Previous versions of CICS had a simpler,
higher-overhead program compression algorithm.  Whenever a
pre-defined storage cushion was used, all unused programs
were immediately unloaded and all used programs were
unloaded as soon as their current usage was complete.  Adding
to the overhead of the algorithm itself was the use of DFP to do
any subsequent reloads of programs.

Maintained in the Loader Domain, the new algorithm has
targets for the amount of DSA and EDSA storage used by
programs not in use.  When either is exceeded, the Least
Recently Used (LRU) approach is used to move out just
enough programs to meet the targets again.  This approach
should eliminate the need for specifying programs as resident,
although requests for residency are still honoured.

As well as reducing the number of program loads required,
Directed Load is much faster, using LLA to Program Fetch
programs already loaded in another address space.


Performance

CICS now manages EDSA directly, rather than using
GETMAIN for each request.  All 31-bit (above the line)
storage requests will be satisfied with storage above the line.
Previously, requests for less than 4K would actually be fulfilled
with storage below the line.

CICS/MVS had six subpools of storage in DSA.  CICS/ESA
3.1.x has 60-70 and 3.2.x has 70-80.  By grouping similar size
areas together, fragmentation can be reduced by wasting less
space when a hole (previously freed storage) is re-allocated.

Because all free storage descriptors are stored together, a
search for the appropriate hole to re-allocate only references a
small number of pages of storage (usually only one).  Compare
this to CICS/MVS which references every page with a hole in
it because each free storage descriptor is kept with its hole.

Quick cell subpools have also been added to locate holes of
particular sizes in just six to eight machine instructions.  Fixed
lengths for commonly used storage sizes like 4K are defined in
this way.

Statistics are provided for monitoring storage management
performance.  Each domain and task subpool has statistics
associated with it.  User storage statistics are subdivided
between above and below the line storage as are overall
statistics.  The following storage statistics are provided:

o      Largest free area

o      Percentage of free storage

o      Total time SOS

o      Times cushion released

o      Number of storage violations

o      Current elements of storage

o      Current pages of storage.

Because statistics are reset every time they are taken, the
current number of elements (pieces of storage allocated) may
not add up to GETMAINs minus FREEMAINs.  A quick way
to determine if VSAM is currently being used is to look for
VSWA elements.

Jon Pearkins
Certified Software Specialists (Canada )

