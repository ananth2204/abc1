Maximizing CICS capacity

This article takes a look at how CICS capacity is limited by its
task management architecture and why MRO fails to address
this problem adequately.  The solution comes from studies in
queueing systems, in the form of a multi-server queue.


CICS CAPACITY

The capacity of a CICS system is a measure of the workload
that it can sustain at an acceptable performance level.  The
capacity limit may be reached for various reasons, the most
common ones being CPU over-utilization, virtual storage
shortage, and real storage shortage.  CPU over-utilization may
occur when there seems to be plenty of spare CPU capacity in
the mainframe.  To understand this paradox, we need to know
how a program consumes CPU.


CICS TASK MANAGEMENT ARCHITECTURE

CICS has its origin as a transaction processing system which
operates as a single operating system task.  Though it has
evolved into a system with multiple MVS tasks, all application
processing is still handled by a main task.

A CICS region has two or more MVS tasks: the CICS main
task, the journal subtask, the external system subtask (when an
external security system is utilized), plus the optional VSAM
subtask.  An MVS task is a unit of work represented to MVS
dispatcher by a Task Control Block (TCB).

CICS applications can use CPU only when the CICS main TCB
is dispatched by MVS.  CICS transactions are threads of work
carried out by internal CICS tasks, each represented by a
Dispatch Control Area (DCA) on a dispatching queue (or task
chain).  Transactions compete for CICS system resources such
as CPU, storage etc, and CICS relies on a priority-based
dispatching scheme to serialize the use of the CPU by
transactions.  Once dispatched by MVS, the CICS main TCB
will not voluntarily give up control of CPU unless there are no
transactions ready to run.


DISPATCHER QUEUEING DELAY

Because a single TCB cannot be concurrently dispatched on
multiple processors, the CICS main TCB can only service one
DCA at any instant, even when running in a multi-processor
mainframe.  Therefore, when a transaction has been
dispatched, all other ready transactions in the active task chain
must wait to be dispatched.  The response time of a transaction
is a combination of queueing delays and service time (CPU or
I/O), and the latter is generally independent of the level of CPU
utilization.  Thus, dispatcher queueing delay lengthens a
transaction's response time.  The impact of queueing delay on
response time is usually negligible until the aggregate
utilization of CICS TCB and all higher priority MVS TCBs
reaches around 40 percent.  At that point it begins to accelerate
exponentially until it reaches a queueing threshold, where the
terminal responses become extremely long and erratic.
Depending on the CPU power and transaction profiles, this
threshold lies somewhere between 50 and 70 percent utilization
by CICS and higher priority workloads combined.


MEASURING DISPATCHER QUEUEING DELAY

Dispatcher queueing delay is not detected by most monitors
because:

1      The completion of an event which renders a transaction
      ready is not always immediately recognized by CICS (eg
      an ECB posted upon file I/O completion is recognized in
      the next task chain scan).

2      CICS does not trace the transition of a transaction from
      not-ready to ready status.

Thus, the detection of dispatcher queueing delay requires
periodic sampling of the active DCA chain.  If your CICS
monitor does not report on dispatcher delay, you might write
your own sampling program.


VSAM SUBTASK PROCESSING

CICS VSAM Subtask Processing (VSP) is meant to increase
parallel processing in multi-processor complexes.  However,
VSP off-loads usually less than 20 percent of the total CPU
required.  Thus, the bulk of the CICS processing  is still
concentrated in the main TCB.  In this respect the capacity of a
CICS region is limited by the power of a single CPU processor.
The architecture of CICS is ill-suited to current high-end IBM
mainframes, which are primarily of multi-processor design.


MRO: A LIMITED SOLUTION TO DISPATCHER QUEUEING
DELAY

CICS offers MRO and ISC as an answer to this limitation on
capacity.  With this facility, multiple CICS regions can work
together, usually with a Terminal Owning Region (TOR)
handling all network traffic and one or many Application
Owning Regions (AORs) for application processing.

Transactions that are remote to the TOR are routed to an AOR
specified by the system-id for the remote transaction.  Since
only one AOR may be specified for each remote transaction,
MRO is inherently a single-server queueing architecture.

Many installations implement MRO/ISC complexes to raise
their capacity thresholds.  In multi-processor CPU complexes,
MRO makes it possible for most CICS regions to operate
below their dispatcher queueing threshold.  When an AOR
consistently gets above its acceleration point at peak load, the
system solution involves reducing the load by:

1      Redistributing the remote transactions among existing AORs.

2      Adding an AOR and redistributing.

In other words we attempt to balance the load among the
AORs.

Load redistribution is not always possible because some
applications may require certain transactions to reside in the
same AOR.  This is the case when they share data in temporary
storage queues or in memory.  Also, any applications that run
as one transaction or have pseudo-conversational transactions
that receive control initially via XCTL canot be split across
AORs.

Taking an extreme example: an MRO complex with one TOR
and one AOR, running a major, non-splittable application on a
3090-600 will reach its capacity limit with only about one and
a half of the six processors utilized.


RELIEVING QUEUEING DELAY WITH LOAD BALANCING

An unbalanced system of AORs is a collection of single-server
queues, with each AOR serving a unique subset of remote
transactions.  On the other hand, a balanced system of AORs
constitutes a multi-server queueing system because a
transaction can be routed to any AOR.  Studies of queueing
systems have demonstrated that in a multi-server queueing
system, queueing begins to accelerate at a higher utilization.
This means that the combined capacity of a number of AORs is
greater in a balanced system than in an unbalanced system.
Furthermore, the former enjoys significantly more stable
performance.

By contrast, IMS/VS implements the multi-server approach.
The same transaction may be scheduled into multiple message
processing regions (parallel scheduling) in such a way that the
work may be balanced between these regions.  This
architecture enables IMS/VS to optimize  utilization of multiple
processors for application processing and therefore to sustain
higher throughput than CICS/VS.  In order to maximize CICS
capacity, we need similar facilities.


THE SOLUTION

At the moment the only option is to install a dynamic load
balancing software package.  (IBM is starting to address this
problem with CICS/ESA 3.1.1.)

Dynamic load balancing software would provide dynamic
routing of transactions to AORs according to established
workload objectives.  This would overcome the single-server
limitation of MRO and enable the creation of balanced, multi-
server systems.  It could push the queueing threshold to over
70 percent CPU utilization.


Murray Turner and Yury Strashnoy
Blenheim Software (UK)                                  c Blenheim Software 1990


