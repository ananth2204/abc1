CICS tuning - avoiding high disk response time

Our purpose here is not to sell you hardware so we will not
discuss disk response time reduction brought about by new
hardware such as 3390 disks (versus 3380) or the
implementation of cache controllers.

Unfortunately, any installations who have their DL/I databases
under the control of DFSMS will not be able to apply the
tuning hint that follows.  The following figures relate to 3380
model D disks, but can be transposed to disks with other
technical characteristics.

Firstly, as we are talking here about DASD I/O to DL/I
databases, the measurements we need cannot be provided by a
CICS monitor - we need other sources.  We can start with a
device report from RMF and look for devices, say, with a
response time above 30 milliseconds.  When the I/O rate to a
particular disk increases (remember queueing theory) the IOSQ
time will start rising accordingly and we may obtain values of
10, 20, or 30 milliseconds, for example.  Total disk response
time is equal to the sum of the normal response time (20
milliseconds) plus queue time, which translates in our case to
30, 40, or 50 milliseconds.

The next I/O arrives before the previous one is finished and,
detecting the device busy condition, it is queued in memory.
RMF can report on these figures so we know which disk packs
suffer from this phenomenon.  As we are tuning CICS we have
to know which workload is generating that high I/O rate to the
disk.  RMF does not tell us which workload it is.  If your
databases are on dedicated packs and CICS is the only
workload accessing them you are one step in advance.  If this
is not the case you need other tools such as a DASD monitor or
a capacity planning tool like CAPTURE/BEST1, which reports
on DASD I/O rate and DASD performance per workload (see
Figure 1).

Your conclusion now may be to move some datasets to other
packs and the problem will be solved.  After the move, to your
great astonishment, you might see that either you still have the
problem on the original pack or you moved the problem along
with the database(s) to the new pack.  So what has happened?
In all probability, the high I/O rate to the pack is for 80% of
traffic (remember the 80/20 rule?) destined to one particular
database and the queueing problem moves around as you move
the database.  Again you need specific tools to be aware of this
because you cannot rely on the (logical) I/O rate reported by a
CICS monitor as that figure may, to a large extent, differ from
the physical I/O rate (consider path calls in DL/I).  A DASD
monitor or, for example, THE MONITOR FOR MVS can
report on the physical I/O rate per job (occasionally CICS) and
per dataset.

The only possible solution here is to split the dataset (database)
across different packs.  How can we do that?

o      Use the possibility provided by DL/I.  The split is based on
      segment types: all occurrences of some or all segments are
      stored in a different dataset.  Therefore a root and some of
      its dependent segments reside in different datasets and that
      may have a rather negative effect on performance since
      more physical I/O is needed to access one database record.
      Another disadvantage is that it needs a separate dataset
      name (and DD statement) for each part of the database.

o      Use the possibility provided by VSAM (if you still use
      ISAM/OSAM for your DL/I databases you should
      investigate in that area), which is based on (VSAM) key
      ranges.  Unfortunately, just how you relate those VSAM
      keys to the key values of your DL/I root segment is still an
      open question for me!

o      We did the splitting based on the allocation mechanism
      VSAM uses while allocating space at DEFINE CLUSTER
      time.  If we need 200 cylinders for  a database we put in
      the DEFINE statement:

         CYLINDERS ( 100 , 9000 )
         VOLUMES   ( VOLSER1 , VOLSER2 , VOLSER3 )

      Initially, we get a primary allocation of 100 cylinders on
      VOLSER1.  When the database is loaded, more space is
      needed and VSAM will try a secondary allocation of 9000
      cylinders on VOLSER1, which of course will fail and
      VSAM will do a primary allocation of 100 cylinders on
      VOLSER2.  VOLSER3 is an emergency volume for use if
      the database fills up during on-line operation and needs
      another extent.

I am sure a number of installations have databases split over
different disk packs but these are most likely large databases
that would not fit on a single disk pack anyway.  Probably very
few, until now, have split databases for performance reasons,
but this might change now.

We applied this technique to four of 75 databases and reduced
overall CICS response time by 50% (ie overall an average
response time of 1.5 seconds was reduced to 1 second).

Fran ois Mortier
Systems and Operations Manager
Atlas Copco Airpower NV (Belgium)

