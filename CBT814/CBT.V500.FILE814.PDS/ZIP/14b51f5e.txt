SSD performance in a paging environment


Introduction

The purposes of this article are:

o      To present details on the performance of a Solid State Disk
      (SSD) used in an MVS/ESA paging subsystem.

o      To describe briefly SSD technology and its potential as a
      DASD.

o      To provide background information on the sources of data
      for analysis of DASD subsystems.

o      To provide background information on the MVS/ESA
      paging subsystems.

A detailed analysis of the performance of a 64-megabyte SSD
linked over a single channel connection to a 3090-170J running
MVS/ESA Version 3 is included. This system had been
experiencing severe performance problems, especially very
high paging rates. Options were needed to resolve these
problems without having recourse to a processor upgrade. A
means of tolerating the high paging rates without affecting
system performance was required.

The SSD was used exclusively for paging. It had been
configured as four logical volumes, each 23 cylinders in size.
A 22-cylinder local page dataset was defined on each volume.
Each of these local page datasets contains 3,300 page slots,
giving a total of 13,200 page slots on SSD. This is
approximately one third of the local page slot requirement of
the production system.

This review takes the form of a detailed analysis of a test
conducted on a single day. This test consisted of moving all
production work, some of which had been running on another
processor mainly used for a development system, back to the
3090-170J and measuring the system performance with the
SSD page datasets in operation.


What is a Solid State Disk?

SSDs contain arrays of storage which emulate standard DASD
subsystems: control units and storage devices. The storage is
configured as a number of logical disk volumes. Because there
are no moving components, the seek, search, and set sector
operations are performed immediately. The control unit
function is integrated within the storage device. A basic SSD
provides one channel attachment, ie the control unit is
equivalent to half a 3990-1 or a 3880-3. The controller
typically adds an overhead of about 0.1 milliseconds to the I/O
operation.

SSD devices are available from various vendors such as
Amdahl (who resell a Fujitsu unit), EMC, Memorex Telex
(who resell a Hitachi unit), and StorageTek. IBM does not
market an SSD device. SSDs have occupied a niche place in
the DASD subsystem. Though not widely installed they can
offer significant benefits. There is very little functional
difference between the units offered by the different
manufacturers. The main differences lie in the minimum size,
maximum size, storage increment, number of logical volumes
supported, number of channel attachments supported, etc.

There is a small overhead in configuring an SSD as many small disk volumes rather than as one large volume. Each volume
must have a label and a VTOC and possibly a VVDS. For files
such as page datasets, which have to be allocated on page
boundaries, this effectively wastes an entire cylinder.
However, this has benefits in separating files and allowing the
use of Seldom Ending Channel Programs. This will be
explained later in this article.

SSDs are generally the best performing I/O devices available.
Their performance is not dependent on data type or data access
patterns. They can transfer data at the maximum channel speed:
4.5 megabytes per second rather than 3.0 megabytes per
second for 3380s and 4.2 megabytes per second for 3390s.
There is no sharing of the SSD control unit with standard
DASD so its performance is not disrupted by other I/O. A great
deal of MVS systems and applications software is still file
oriented: data is stored in and read from physical files, which
must reside on disk. SSDs can be useful in these circumstances
as they allow very high I/O rates to be tolerated without
significantly affecting performance.

SSD devices are capable of sustaining very high I/O rates and
still delivering consistent and low response times of around 3-5
milliseconds. Performance tends not to be degraded by high
utilization.


Issues in DASD Subsystem Performance

The main components of a DASD I/O request are:

              Element      Typical duration (milliseconds)
Queueing for channel access      3.0
Protocol overhead in the      1.5
DASD control unit
Seek time when the disk      5.0
read/write heads must move
to locate the cylinder where
the data resides
Latency when the heads must      7.0
wait until the disk spins so
the data is located under the
heads
RPS miss where the channel      Variable
availability and the data
availability do not match so
the disk must complete a further
rotation
Transfer time where the data      1.0
is read or written

DASD I/O service time is the sum of the following
components:

o      Protocol
o      Seek
o      Latency
o      RPS miss
o      Transfer.

That is, it is the time taken for the DASD to present the data in
response to an I/O request.

DASD I/O response time is the effective I/O time as seen by
the program and consists of service time and any queueing time
which may have occurred. The main components are those
associated with the mechanical movement of the disk where the
heads must seek to the correct cylinder address and then wait
for the disk to rotate so the data is located under the heads.

Essentially there are four alternatives available in reducing the
amount of time taken for data to be transferred between disks
and the CPU where it can be operated on.

Of these, three involve improving the DASD subsystem
performance by adding components which offer greater
performance, while the fourth involves reorganizing the
existing DASD. The choices are:

1      Reduce the physical components of DASD I/O by adding a
      form of data memory at the control unit end of the data
      channel. Frequently accessed data is stored in the memory
      and transferred to and from it when required. This will
      eliminate the need for the physical access of disks. There
      are two options here:
      -      SSD.
      -      Cache memory which is present in the DASD control
            unit where frequently accessed data is stored.

2      Use facilities provided the MVS operating system to avoid
      I/O operations by pre-loading data into virtual storage.

3      Improve the physical characteristics of the DASD so that
      they seek and rotate faster and transfer data more quickly.

4      Tune the DASD subsystem by balancing I/O between and
      across volumes and strings so that the I/O load is evenly
      distributed.

These options all have advantages and disadvantages.


Cache

Advantages

o      Good overall performance improvement
o      Easy to install and use.

Disadvantages

o      Can be dependent on data read/write ratios
o      Can require some tuning to optimize performance.


Solid State Disk

Advantages

o      Offers very good specific I/O performance
o      Not dependent on data access patterns
o      Can sustain very high I/O rates.

Disadvantages

o      Applicable only to small amounts of data.


I/O Avoidance

Advantages

o      Uses native operating system functions.

Disadvantages

o      Requires time to implement
o      May require application redevelopment.


Faster DASD

Advantages

o      Can offer other benefits: reduced floorspace, maintenance
      costs etc.

Disadvantages

o      Can be very expensive
o      Requires data migration (eg 3380 to 3390)
o      Limited improvement without another option such as cache.


Tuning

Advantages

o      Can offer overall improvement.

Disadvantages

o      Requires substantial effort
o      Can require additional software
o      May not be feasible
o      Limited improvement achievable.

To exploit expanded storage fully requires use of the
MVS/ESA address space functions. Data is stored in
dataspaces, which are held in virtual storage (central,
expanded, and auxiliary storage). Expanded storage provides a
backing storage medium to central storage. Expanded storage is
effectively only used for paging. Unfortunately, very little
third-party software makes use of the MVS/ESA address space
facilities.

The use of DASD cache is based on the principle that once a
record is accessed there is a high probability that it or records
located near it in that file will be accessed within a short period
of time. To work effectively cached files must have both a high
ratio of reads to writes and good locality of reference.
Randomly accessed files or those frequently read may
experience less improvement in access times. Also, files which
are accessed infrequently will have their cache slots given to
other, more frequently accessed files.


Solid Stage Disk and Paging

MVS/ESA attempts to use the I/O suspend/resume facility for
paging operations. The Start Subchannel (SSCH) channel
command is used to start the I/O. There is an overhead in
building the required Operation Request Block (ORB) and
other control blocks. When the paging operation has been
completed, the channel program is suspended. It can then be
resumed at a later stage using the Resume Subchannel (RSCH)
channel command. The existing control block structure can be
used so there is less overhead. The result is a Seldom Ending
Channel Program (SECP).

However this is disrupted when the disk is shared and I/O
operations to other files are initiated. SSD can therefore be an
ideal candidate for paging spaces. They can be dedicated
exclusively to paging. The ability to configure the storage
provided by an SSD as a number of logical volumes can be
used to place paging and normal datasets on separate volumes.
Paging is very important in MVS/ESA and its performance is
central to the performance of the system.

Essentially MVS/ESA assumes that devices will be used
exclusively or almost exclusively for paging. This is implicit in
the design of the MVS paging subsystem. If this is not the case
then performance problems can and will occur.

Virtual storage in MVS/ESA

The virtual storage configuration of MVS/ESA consists of three
components:

1      Central storage (CSTOR)
2      Expanded storage (ESTOR)
3      Auxiliary storage (ASTOR).

To be operated upon, data must be in CSTOR. ESTOR can
only be directly accessed using special dataspaces in MVS/ESA
called Hiperspaces. Apart from these, ESTOR is used as a fast
paging area for CSTOR. Data is transferred to and from
ESTOR at about 300 megabytes per second in 3090-type
processors. This compares with the maximum speed of 4.5
megabytes per second at which data can be transferred to and
from ASTOR. ESTOR pages have to be migrated to and from
ASTOR via CSTOR.

MVS/ESA uses ESTOR to buffer data to reduce and schedule
I/O operations. Page faults, which are resolved by transferring
a page from ESTOR, are satisfied synchronously, ie there is no
I/O interrupt and the task is not suspended. It is similar to a
queued access method I/O where the data is already in the
program buffer.

Auxiliary storage in MVS/ESA

All data in virtual storage must be on auxiliary storage.
Therefore, as the use of virtual storage increases with, for
example, dataspaces, the auxiliary storage requirements of the
system increase.

A 3380K or 3390-2 volume can store 398,250 page slots.
MVS/ESA allows local page datasets to be up to 16 million
slots in size. A 2-gigabyte dataspace would therefore require
one and a third volumes to store all its page slots.

The architectural limit of 16 terabytes on the virtual storage of
an MVS/ESA address is substantially more than the current
physical limit of 393 gigabytes. Therefore, MVS/ESA can
mean substantially more and not less auxiliary storage.

I/O avoidance in MVS/ESA

MVS/ESA offers a number of schemes for avoiding I/O
operations. In many cases standard file I/O is replaced with
paging operations. An initial burst of I/O at program start is
needed to load the data into virtual storage. Thereafter paging
operations are performed.

MVS/ESA requires a high performance paging subsystem. All
the dataspaces must reside in virtual storage. Dataspaces can be
private (restricted to only a single user) or public (shared
among a number of users). Private dataspaces are swapped
with their owning address space.

Loading data into virtual storage, where it is operated upon,
may have implications for data integrity. If updates are not
performed on the original disk file, they may be lost in the
event of system failure. Dataspaces should probably only be
used for read-only data.

Potential implications in paging from expanded storage

MVS is an interrupt-driven operating system. When a high
priority task is interrupted, lower priority tasks receive service
and so on. Most of the interrupts are caused by I/O.

Paging I/O from ESTOR is synchronous and does not result in
an interrupt. This will mean that there will be fewer interrupts
in MVS/ESA. Therefore low priority tasks will receive CPU
service less frequently and high priority tasks will be allowed
to dominate the CPU. This may result in increased elapsed
times for long transactions and increased elapsed times for
batch jobs.

Paging control blocks in MVS/ESA

MVS maintains two sets of control blocks describing the
paging subsystem. The PART and PARTE are used for page
datasets. The SART and SARTE are used for swap datasets.
They can be located from the CVT as shown in Figure 1.

The PART/SART divides page datasets into three categories:
those residing on cache controllers, those on devices with a
fixed head feature (3350s, 2314s), and those on normal DASD.
Page datasets are selected in this order - as most page datasets
reside on normal uncached DASD this is not relevant. Each
page dataset is described by a PARTE/SARTE.

The PARTE/SARTE contains information on the performance
of a page dataset. The PAT/SAT is chained off the
PARTE/SARTE. MVS will select the best performing page
datasets and will favour them over poorly performing page
datasets. Thus there is an automatic mechanism within MVS to
exploit SSDs for paging. The PARTE also records the last slot
referenced and attempts to resume access of the page dataset
from that slot: it assumes that there has been no head
movement. If the disk has been used in the interim, this
assumption fails.


SSD Usage and System Performance

Information is provided in four SMF records. Three are written
by RMF and one by SMF. The RMF records are written at the
end of the RMF interval.

SMF record types

Type 30 subtype 4 (or type 4) - step termination

This contains information on resource usage and other
information for a job step. It is written at the end of the job
step.

RMF record types

Type 71 - paging activity

This contains information on paging activity, paging rates, and
real storage usage.

Type 72 - workload activity by performance group

This contains information on resource usage for all
performance groups.

Type 75 - page dataset activity

This contains information on the usage of page datasets.

The information required to evaluate the SSD and the sources
of that information are:

o      Page dataset usage (type 75) - look at the size of dataset
      and the maximum and minimum number of slots used and
      the number of pages transferred during the interval.

o      Workload activity (type 72 and type 30-4) - look at the
      MSO units for each performance group. This will indicate
      the real storage usage by time of day. Use the Type 30-4 to
      get the start times of the jobs steps of the important jobs
      and started tasks: CICS systems, IMS regions, VTAM etc.
      Make sure that JES2 and RMF are fully initialized.

o      Paging activity (type 71) - this just gives the overall
      system paging rate and other measurements.


Analysis of SSD Test

The data used in the following analysis is taken from the RMF
reports produced during the test. RMF data was written at 15
minute intervals. Some detail was lost because the level of
'granularity' is not sufficiently fine to indicate major peaks in
paging and other measurements of system performance. The
values are averaged over the 15 minute interval.

The SSD contained 64 megabytes of storage and was
configured as 4 logical volumes, each 23 cylinders in size. A
22 cylinder local page dataset has been defined on each
volume. Page datasets have to be allocated on cylinder
boundaries. The first cylinder was used for the volume label,
VTOC, etc. Each of these local page datasets contains 3,300
page slots, giving a total of 13,200 page slots on an SSD. This
is approximately one third of the local page slot requirement of
the system.

A characteristic of IBM operating systems is that high paging
rates result in low CPU utilization. This occurs because
portions of the MVS system and its service tasks such as
VTAM, JES2, etc are themselves subject to paging. During
periods of high paging, pages are stolen from MVS. The
system consequently has to wait for its pages to be paged in
before it can drive the CPU.

This is not the phenomenon called 'thrashing', which occurs in
other operating systems that implement virtual storage
differently. This is high CPU utilization caused by the system
having to devote a large part of its resources to servicing
paging requests from active tasks.

Paging in MVS is very efficient. The instruction path to
perform a page-in or page-out operation is quite short: only
2000 to 3000 instructions. Therefore, servicing a paging rate of
350 experienced by the system would take 1.05 MIPS.

An SSD solves this problem by allowing paging to be
performed extremely quickly. High paging to mechanical disks
causes the devices and the channels which link them to the
CPU to be highly used. This results in very high device
response times caused by RPS misses in the disk and queueing
in the channel. This further delays the servicing of paging
requests.

SSDs service I/Os at a constant rate of 4 to 5 milliseconds
irrespective of their rate of use.

Figure 2 shows the values for CPU percentage busy and paging
rate for the 15 minute intervals from 9.30 to 14.45.

Note that the paging rate given here is not the same as the
demand paging rate given in the RMF summary report. This
latter value is the rate at which pages are requested. To service
these requests, pages have to be freed, their contents written to
auxiliary storage if changed, and the requested pages read from
auxiliary storage. Also, some page reclaims may occur.

In the analysis which follows the four RMF intervals ending at
11.45, 12.00, 14.00, and 14.15 are examined in detail and are
marked in the table. These represent the morning and afternoon
peaks when the paging rates were at their highest.

This ability to allow high CPU utilization during periods of
high paging is shown in Figure 2, which presents both the CPU
percentage busy and paging rates. The two sets of values
closely follow each other. The very high paging rates at 11.00
to 12.00 and 14.00 to 14.45 are matched by high CPU
utilization. The SSD has therefore enabled the CPU to be used
more efficiently even though paging is very high.

Figure 3 shows the local page dataset configuration in
operation during the test. There were 11 locals in all, 4 of
which were on SSD. The SSD represents 13,200 slots or 19%
of the total available 69,450 slots.

Figure 4 shows the slot usage for the local page datasets for the
four chosen RMF intervals. The SSD local page datasets were
always very highly and consistently used with their maximum
and average slot usage values very close. At peak periods, all
their 13,200 slots were fully occupied. This represents just
under 35% of the total slots used. MVS requires that the total
local capacity is always less than 70% full, but it will favour
high performing page datasets within the paging configuration.

This clearly happened during the test where the significantly
better performance of the SSD locals was recognized and
exploited by MVS.

Figure 5 shows the number of I/O requests to the 11 disks that
were for paging operations. At peak times, the SSD is handling
nearly 47% of the total paging I/O operations. The SSD
received significantly more I/O than any other disk even though
the page datasets defined on them were smaller than any other
local page dataset. This indicates that not only did MVS fill the
SSD locals, it filled them with the most active pages to take
advantage of the performance improvement they could yield.

Figure 6 contains detailed information on the performance of
the disks, which contain local page datasets. The columns in
the table are:

o      I/O RATE - this is the average I/O rate per second for the
      disk.

o      RS-TM - this is the average time in milliseconds for the
      disk to service an I/O request.

o      I/O TOTAL - this is the total number of I/Os to the disk
      during the interval.

The SSDs exhibit a constant response time of 4 to 5
milliseconds. The other disks are subject to variation and the
response time increases with the utilization as follows:

         Volume         Response time variation
         DISK04                31 to 73
         DISK02                29 to 33
         DISK03                26 to 30
         DISK07                27 to 30
         DISK01                25 to 29
         DISK05                30 to 41
         DISK06                29 to 33

The SSDs are sustaining I/O rates 40% to 50% higher than any
of the other disks even though the I/O to the SSDs is all
paging-related while I/O to the other disks contains a small
non-paging component.

The I/O rate to the four SSD volumes linked over a single
channel connection represents 45% of the total paging I/O rate
to all eleven page datasets. These paging I/Os are being
serviced 85% to 90% faster than those to the other volumes.

Figure 7 gives the performance of the channel paths linking the
paging DASD to the CPU. The data in the table is classified by
logical control unit. See Figures 2 and 3 to relate this to the
page datasets and disks.

The I/O RATE is the average I/Os per second for all DASD
linked to the given LCU.

The CH PATH % BUSY column gives the percentage of the
interval all the channel paths to the LCU were busy.

The % I/O DEFER column contains the percentage of I/Os that
were deferred because all channel paths were busy.

Even though the channel path to the SSD (on LCU 020) was
significantly busier than those to the other paging DASD, the
percentage of I/Os that were deferred is much lower than for
these other DASD. This happens because the SSD is able to
service I/O requests more quickly than normal DASD and there
is therefore much less queueing of I/O requests.

Figure 8 shows the Main Storage Occupancy (MSO) units
accumulated by the performance groups for the major
subsystems for the four intervals. One MSO unit is equal to 50
pages being held for one CPU service unit. The 3090-170J can
deliver 811.47 CPU service units per second. Therefore, one
3090-170J MSO unit is equal to 0.0616 pages per  second.

This can be used to derive an estimate for the average working
set for a task. However for this estimate to be accurate there
must be only one active task for each performance group. In
this case, performance groups 22, 25, 34, and 35 represent
more than one active task. Performance groups 22 and 34 are
significant users of storage.

The working set estimates are only one measure of storage
usage.  CSA, storage required by the resident MVS supervisor,
page and segment tables, etc are all users of main storage.


Summary

The results of the SSD trial show:

o      The SSD provided 19% of the total available page slots in
      the system.

o      During the periods of peak system utilization, the SSD
      slots accounted for 35% of the used slots.

o      At peak times, the SSD page datasets accounted for 47%
      of the total paging I/O load.

o      These were serviced 85%-90% faster than the other paging
      operations.

MVS/ESA recognises page datasets which perform well and
uses them more often than page datasets which perform less
well. This mechanism was clearly evident from the results of
the trial. While the SSD provided only 19% of the available
auxiliary storage, it accounted for nearly half of the paging
load. MVS favoured the SSD, not only by fully utilizing the
space it provided but also by using it to hold the busiest pages.

Using a SSD for paging allows very high paging rates to be
tolerated while not affecting overall system performance. In the
specific case documented here, using a small SSD allowed the
processor to be more fully utilized.

Alan McSweeney
Technical Director
Icon Systems (Ireland)                                          c Xephon 1993

