Blocksizes for partitioned datasets

Everyone knows that maximum blocksizes for sequential datasets 
can dramatically improve I/O performance.  However, with 
partitioned datasets, the question arises as to what is actually the 
'optimum' blocksize.  Part of the problem stems from an implicit 
belief that there is something significant about a 6K size, 
especially since most DLIBs and CBIPOs come distributed in 3K 
or 6K blocksizes.  This arbitrary 'standard' is used only because it 
provides the most compatible size across different device types 
(eg 3330, 3350, 3380 etc).

For fixed-length records, such as PROCLIB, MACLIB etc, the 
most efficient blocksize is that which maximises track utilisation.  
That the average size per member may be smaller than this makes 
no difference.  Specifying a large blocksize ensures that inter-
block gaps per member are minimised, with the result that you 
will be able to store more records per track, just as in sequential 
files.  If all the members are very small, for instance one or two 
records per member, there will not be much difference, no matter 
which blocksize is chosen, since the file is essentially unblocked.  
Also, if there is a large difference in the sizes of members, 
selecting an average for a blocksize will serve better, as large 
members next to small members may not fit on the balance of a 
track, resulting in wasted space.

The effect of blocking in load libraries is more difficult to 
determine, primarily because of an inadequate understanding of 
the internal structure.  When a load module is created, the 
Linkage Editor writes records to describe important information 
used when the module is to be executed.  External Symbol 
Dictionary (ESD) and Relocation Dictionary (RLD) records 
surround the text and enable Program Fetch to properly load the 
program to run.  ESD records are created for each CSECT and 
unnamed control section, while RLD records define every 
relocatable address constant, its location, and the ESD entry used 
to compute its value.  These records are variable length, 
depending on the number of entries, up to a length of 
approximately 256 bytes.  Text records are also variable length, 
but have several factors that can influence the maximum blocksize 
actually used by the Linkage Editor (see below for a more detailed 
explanation).  It is important to note that a CSECT is never split 
across text records that contain other CSECTs.  In other words, 
text records will always contain either complete CSECTs or a 
truncated CSECT continued from a previous text record.

The effect of blocking, apart from device utilisation, can affect 
Program Fetch performance.  Small blocksizes will not conserve 
storage, since 96K is always fixed for Program Fetch buffers, but 
it may adversely affect performance.  When a program is fetched, 
one text record and up to 48 RLD/control records are read in each 
I/O operation.  PCI interrupt is used to add CCWs dynamically 
and having larger text blocks allows more time for this activity.

By far the most important factor affecting performance is 
maintaining valid RLD counts, so that the correct number of 
CCWs can be chained together.  Invalid counts result in Program 
Fetch getting an I/O error and restarting the entire load operation, 
reading each RLD record individually instead.  This is easily seen 
as increased 'device busy' on performance reports.  Valid counts 
are provided by the DFP Linkage Editor or by using the 
ALTERMOD or COPYMOD functions of IEBCOPY.

As can readily be seen, specifying blocksizes for load libraries is 
not as simple as it is with sequential files, with a significant role 
played by the Linkage Editor in determining the actual results.  In 
moving libraries between different device types it is advisable to 
use the COPYMOD facility of IEBCOPY to maximise the benefit 
derived from the new device.

FACTORS AFFECTING BLOCKSIZE

The Linkage Editor will determine the maximum text sizes used 
by several factors.  Since maximum blocksize equates to 
improved fetch performance, the following information should be 
reviewed to ensure that the results of blocking are those which 
you expect.  This list is not in any priority order, since any one of 
these factors may, by itself or in conjunction with other options, 
affect the final results.

1	Device type:

	3330 - 12K maximum text size

	3350 - 18K maximum text size

	3380 - 32K maximum text size.

2	DSCB information: this is determined by the blocksize 
specified during library definition.

3	JCL information: this refers to the value coded on the 
SYSLMOD statement.  It will not be used unless the DSCB 
option is coded.

4	DSCB option: used to override original DSCB specification.

5	DC option: instructs the Linkage Editor to construct text 
blocks using 1K or less.

6	REGION and SIZE: if value is too small, inadequate buffer 
space may be allocated, resulting in smaller text records.

7	SCTR option: this is no longer used in VS systems except for 
Nucleus links.

8	SYSUT1 blocksize: if a different device type or blocksize is 
specified, this will affect the blocksize written to SYSLMOD.

9	Copying (IEBCOPY): if IEBCOPY is used to copy modules 
across different device types, reblocking will not occur unless 
the COPYMOD function has been used.

Note: sometimes there is concern expressed that 32K for 3380 
devices is inefficient or wasteful.  It is important to remember that 
the majority of records will be substantially smaller than this size 
and the criteria normally used for datasets cannot be applied.


Gerhard Adam
Technical Consultant (USA)	© Gerhard Adam 1989

