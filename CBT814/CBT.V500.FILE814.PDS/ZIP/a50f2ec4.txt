Misleading information in RMF

INTRODUCTION
Normal tuning often starts with a look at the reports generated by
RMF or some other performance measuring tool.  Whenever an
I/O subsystem is analysed, potential bottlenecks are looked for.
Potential bottlenecks are recognised through the length of the
queuing time.  This is the time an I/O request waits until it can be
served.  In an RMF report it is recorded under the heading IOSQ.
But can we truly relax if our RMF report shows IOSQ to be zero?

My approach is that whenever you face high disk I/O rates and in
turn high device utilisation, your red light threshold should start
blinking.  Although you may find the I/O response time is
acceptable, it is not unheard of to find users complaining of bad
response times at their terminals.  Before accusing the network,
I’d suggest you investigate your on-line system (IMS, CICS,
IDMS etc) or any other DBMS server.

In any of these systems you may find a high disk I/O rate is
caused by a single file.  This can easily be identified by looking at
the number of open datasets (RMF DASD report).  In such a case
the report shows a value of 1.  Such a situation affects all on-line
users.

INTERNAL QUEUING
Even with IOSQ=0 there could be lots of I/O requests which are
waiting inside the on-line system address space rather than in the
I/O supervisor queue.  Clearly the reason for waiting inside is
because the requester (IMS, CICS, IDMS etc) will never issue
more than one I/O at a time to the same file unless a previous I/O
has completed.  Therefore all other I/Os beyond the one currently
being executed will be forced to wait inside the address space
requester and so affect every on-line user.

PERFORMANCE DEGRADATIONS
A long chain of tasks will be forced to wait until the file becomes
available.  This file might be the CICS temporary storage file, the
IDMS SCRATCH file, or some similar type of file.  While
waiting for the I/O to complete, each task still occupies its related
storage.  This in turn causes virtual storage to be constrained even
in MVS/XA.  Another concern is that the maximum task threshold
will be hit more frequently.  Any of these conditions will, of
course, cause performance degradation.

SYSTEM-WIDE VIEW

What we have seen so far is that the RMF report is not sufficient
to analyse a performance problem.  The system-wide view should
take into account both internal and external aspects.  External
analysis can be accomplished through the RMF report.

Figure 1 shows how I/Os are normally queued in the I/O
supervisor.  As can be seen, each address space issues its I/O
request regardless of the device activity.  The I/O will be queued
in the IOSQ along with other I/Os.  I/Os are performed one after
another as determined by the I/O supervisor.

RMF reports the average time each I/O request has to wait (IOSQ)
until it can be performed.  Whenever a high I/O rate is
encountered without an I/O queue but with end users reporting bad
response times an internal approach to the problem should be
adopted.  This internal analysis should examine who is causing
the high I/O rate.  If it is caused by an on-line system or database
server we can be pretty sure that the queue is concealed inside the
address space.  An appropriate report can prove that.  Figure 2
illustrates where the queue is hidden.

As can be seen, a particular address space such IMS, CICS,
IDMS etc will never issue more than one I/O at a time to the same
file.  Therefore the queue will be managed inside the address
space.  When an I/O request is successfully completed the next
one will be immediately performed by the same address space.
This is why a device can be heavily utilised without any signs of
external queuing.

A REAL-LIFE EXAMPLE
The following example demonstrates a system-wide view.
According to RMF and IDMS reports the activity was as follows:

1   Transaction rate - 5 per second
2   I/Os per second (to one volume) - 40
3   I/O service time - 20 milliseconds
4   IOSQ time - 0
5   Device utilisation - 80%
6   Number of datasets open - 1.
The high I/O rate was caused by the IDMS SCRATCH file.
From the above, we can determine that:
•   Average I/Os per transaction: 40/5 = 8 (from 1 and 2 above).
•   Average response time per I/O: 20/(1-0.80) = 100
milliseconds (based on queuing theory formula).
•   Average I/O response time per transaction: 100 x 8 = 800
milliseconds.
The I/O response time relates only to the SCRATCH file.  In this
case it is approximately 0.8 seconds.
What if … ?

How will response time be affected if the transaction rate is
increased by 20%?
•   New transaction rate: 5 x 1.2 = 6 per second.
•   I/Os per second to scratch file: 6 x 8 = 48.
•   New device utilisation: 48 x 20 = 96%.
•   Expected average I/O response time: 20/(1-0.96) = 500
milliseconds.
•   Expected I/O response time per transaction: 500 x 8 = 4000
milliseconds.
CONCLUSION
As can be seen, the average I/O response time per transaction will
be about 4 seconds.  This time excludes CPU time and other
related I/Os such as database I/O etc.  This means that an end user
will experience a response time of 5 to 6 seconds.  Even if this
poor response time is acceptable (which I doubt), the high device
utilisation (96%) means that there is no room for growth potential.
This means that although the CPU is under-utilised, it can’t be
used.  If the CPU is over-utilised, a CPU upgrade is the only
solution.  Actually, without resolving the SCRATCH file
bottleneck, the transaction rate cannot be increased nor can the
response time be improved.

A possible solution to this problem would be to install a sold state
disk such as a NAS 7900.  Its response time is less than 2
milliseconds for a 4K block size.  Given our previous example we
could expect the following results:

•   I/Os per second: 48.
•   SSD utilisation: 48 x 2 = 96 milliseconds = 0.96% (say 1%).
•   Expected average I/O response time: 2/(1-0.01) = 2.02
milliseconds.
•   Expected I/O response time per transaction: 8 x 2.02 = 16.16
milliseconds.
By using such a device the I/O response time would be reduced
from 4 seconds to 0.016 seconds.  The transaction response time
would be reduced from 4 seconds to half a second.

By taking a system-wide view, we were able to pinpoint the real
problem and to achieve acceptable response times.


Emmanuel Vazana
Systems Engineer (Israel) © Emmanuel Vazana 1989

