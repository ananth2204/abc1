Designing a VM system DASD layout

INTRODUCTION

This article attempts to provide a VM systems programmer
with guidelines for planning the physical layout of the system
areas and user mini-disks within a VM system.  The guidelines
are easier to use if a new VM system is being installed and
tested.  This is because the layout of areas and mini-disks is
potentially easier to alter (particularly on the SYSRES pack)
during the re-installation of a VM system.

However, they can also be used for planning the reorganization
of a current production system layout, but because the areas
and mini-disks may be in constant use, more steps could be
required to complete the reorganization without causing a
disruption of service to the VM/CMS user base or any guest
operating systems running under the VM system itself.

This article was written after a full reorganization exercise had
been carried out on a VM/XA SP system and is therefore aimed
primarily at that level of VM.  However, because the majority
of the areas and mini-disks mentioned in the article can also be
found in VM/SP VM/HPO and VM/XA SF systems, the
recommendations also apply to those systems where similar
areas and types of space can be found.

Part one of the article makes recommendations for the placing
of the areas and mini-disks to achieve performance benefits and
discusses other issues that are related to VM system
performance

Part two discusses the concept of a 'system-only' SYSRES
pack, and explains the possible benefits to be gained from this
in the areas of installation, maintenance and recovery
procedures.

POSITIONING FOR PERFORMANCE

What is DASD seek time ?

One of the main overheads on any operating system is that
caused by disk head movement.  This occurs when the disk
head passes over the disk to reach the position at which a
READ/WRITE has been requested by an application program
or the operating system itself.  The time taken to complete this
movement is known as DASD seek time.

While DASD seeking is taking place, nothing else on the disk
pack can be accessed and so the longer the time that is spent
simply moving the head from one part of a disk to another, the
longer the delays will be for other access requests to the same
disk to be satisfied. It is impossible to eliminate the overhead
caused by DASD seeking.  When designing the DASD layout
of VM system areas and mini-disks, the designer should
attempt to reduce as much as possible the amount of head
movement that will be required by the operating system to
continue its operating functions.

How can the amount of head movement be reduced ?

Head movement can be reduced by placing the most often
accessed system areas and mini-disks around the centre of the
packs available to the VM system. This has the effect of
concentrating the head movement around the centre of each
pack and thereby limiting the number of requests to access
outer portions of the packs to a very small percentage of the
total number of requests made.

Paging areas

All VM systems do paging and have PAGE areas defined.  The
performance of a VM system heavily relies on the efficiency of
its paging.  This efficiency is mainly affected by the following
three factors :

%   Number of PAGE areas defined to the system.

%   Size of the PAGE areas defined.

%   Positioning of the PAGE areas on the packs.

All three of these factors affect the speed at which the system
can satisfy a paging request and therefore the efficiency which
which paging can occur.

It is important to place all VM PAGE areas as near to the
centre of a pack as possible.  This will mean that the maximum
distance a disk head will have to travel to satisfy a paging
request (regardless of its current position over a disk) is
approximately half the length of the pack (plus half the length
of the PAGE area).

See also 'Other paging/spooling considerations'.

Spooling areas

Spooling areas are used to store files that are 'held' in CMS
users' virtual readers/printers/punches.  The  size and number
of SPOOL areas to be defined usually depends on the number
of CMS users in the VM system and the number of spool files
that they wish to maintain.

The performance requirements for accessing SPOOL areas is
also governed by the size of the CMS user base.  If good
performance is required when accessing the areas, or a high
number of I/Os is expected to be made to them, they should be
placed in the centre of the packs on which they are defined.  If
performance is not a consideration and there will be a limited
number of I/Os expected to the SPOOL areas, they should be
placed as near to the outsides of the packs as possible.  This
would probably be the case if only a small CMS user base has
been defined because the VM system was only installed to
support multiple guest operating systems on the same CPU.

The CMS operating system can be optionally saved in a Named
Saved System (NSS) which is stored in the SPOOL area.  If
there are a large number of CMS users defined to the system
who frequently log on and off and who IPL CMS from the NSS
instead of from MAINT's 190, this will significantly increase
the number of requests to access the SPOOL area.

If the PAGE areas defined to a VM system become fully
utilized and VM still requires more paging space to handle it's
current paging requirements, this additional requirement is
satisfied from any unused spool space defined to the system.
Paging to spool space is not too uncommon (depending on the
amount of PAGE space defined !!) and can happen at peak
workload times or often when a guest VSE or MVS machine is
being IPLed under the VM system.  Because of this, it is
sometimes better to simply allow the system to page to the
spool space for the period of the 'peak', rather than define
additional paging space that will be under-utilized for perhaps
95% of the time that VM is running.  Bear in mind however,
that performance will be poor at peak times anyway and it may
not be desirable to have the VM system page to a SPOOL area
at a time when it is already struggling to cope!

Paging to spool space is less efficient than paging to regular
page space, so if you have to allow your VM system to
occasionally page to spool space, you might want to ensure that
the spool space is defined in the centre of the packs.  This will
at least limit the overhead of paging to spool space as much as
possible.

See also 'Other paging/spooling considerations'.

Other paging/spooling considerations

If you read articles on VM performance, you will often find
recommendations to place PAGE and SPOOL areas on separate
packs and also spread them evenly across channels.  These
recommendations are undoubtedly correct, but it is unlikely
that the VM system will have enough packs defined to it and
spread evenly across several channels to allow you to fully
implement those recommendations.

The best you can do is to implement them as far as your current
VM I/O configuration will allow.  Even for small VM systems,
you ought to be able to separate PAGE and SPOOL spaces
onto different volumes.  If this is not possible because of the
limited amount of VM volumes available, you should at least
attempt to place PAGE areas on separate packs, as it is not
desirable to have more than one paging area defined on a pack.
It is better to have one PAGE and one SPOOL area on each of
two packs, than two PAGE on one pack and two SPOOL on
another.

It is also better (if possible) to define PAGE areas 'small-and-
many' rather than 'large-and-few'.  Paging becomes less
efficient if the areas defined are too large.  The smaller the area
defined, the less work the paging system has to do to maintain
the internal references to where a particular page is located
within a paging area.  Conversely, care must be taken to ensure
that the areas defined are not too small (particularly with VM
systems that use the block-paging method of paging) as this can
also have a detrimental effect on the efficiency of the paging
system.

Another recommendation that you may come across is to place
PAGE areas on their own packs with nothing else allocated.
Once again this is unlikely to be feasible unless you have an
excess of spare disk space!  As a compromise, you should be
able to place low-usage mini-disks on the paging packs (see
'MAINT source disks etc') in an attempt to limit the number of
non-paging I/O requests to a pack with a paging area on it, and
also to balance the I/O across all the VM packs.

VM user directory

The VM user directory mentioned in this section is the one that
is on-line to the VM system and is stored in the $DRCT$ area
which is defined when the VM system is created.  This section
does not refer to the VMUSERS DIRECT source file that is
stored on one of MAINT's work disks.

The frequency with which the on-line directory is accessed
during a VM session is dependent on the size of the CMS user
base.  The directory is not just accessed for LOGON requests,
but is also accessed, for example, each time a CP LINK, CP
SPOOL PUN, or CP MSG USERID command is issued.  The
directory is searched to ensure that the user-id specified in the
command is a valid CMS user-id.

Therefore, in a heavily used CMS environment, the directory
can be accessed on a regular basis and would need to be placed
towards the centre of the pack on which it was defined.

VM checkpoint area

The checkpoint area for VM ($SYSCKP$) is used to
'checkpoint' the VM spool files on a regular basis so that
recovery of the spool files is possible in the event of a WARM
or FORCE start IPL of the VM system.  Place this area near
the centre of the pack, unless you have a very low-usage CMS
system.

VM warm start area

The VM warm start area ($SYSWRM$) is only accessed when
the VM system is being shut down in an orderly manner (via
the SHUTDOWN operator command).  Information required to
complete a WARM start at the next IPL is stored in this area
just prior to the final shut-down.  Because of this, the warm
start area can be placed towards the outside of the pack that it
is defined on.

CP nucleus area

This area is defined on the VM SYSRES volume to hold the
CP nucleus which is created from CP program modules that are
stored on MAINT's source disks.  The nucleus area ($CP-
NUC$) is only accessed when a new nucleus is being generated
and at VM IPL time when the nucleus is read into storage.  It is
therefore recommended that the $CP-NUC$ area is placed on
the outside of the SYSRES pack.

There is another consideration for the placing of the $CP-
NUC$ area that does not concern performance and is often
overlooked.  If you are running VM/XA SF (Release 1.0 or
2.0) or VM/XA SP (Release 1.0 or 2.0) there is no facility in
the HCPSYS file to specify the size of the CP nucleus area -
you can only specify the starting cylinder.  The actual size of
the nucleus is dependent on (among other things) the size of the
real I/O configuration that you have defined to the system in
the HCPRIO file.

When the nucleus is written to the $CP-NUC$ area at nucleus
generation time, the generation program will use as much space
as required to hold the nucleus.  If the nucleus requires more
space than has been defined for the $CP-NUC$ area, the
generation program will continue writing the remainder of the
nucleus over the next cylinder following the $CP-NUC$ area.
If this occurs, the generating program will not issue a message
to say that it has run over the end of the $CP-NUC$ area.

This is not a serious problem if the next cylinder after the $CP-
NUC$ area is not in use.  However, if it has been defined as a
CMS mini-disk, this can have a potentially disastrous effect.
The original data on the mini-disk that was overwritten by the
new nucleus will of course be corrupted.  More seriously
however, if the mini-disk is subsequently written to by a CMS
user, this may in turn corrupt the portion of the nucleus that
had overflowed the $CP-NUC$ area and will almost certainly
render the nucleus unusable.  As mentioned earlier, the disk
copy of the nucleus is only read when the VM system is IPLed
and so the corruption of the nucleus will not be discovered until
the next time the VM system is IPLed.  Depending on the
frequency of VM IPLs, it may be some time before the error is
detected.

If back-ups of the nucleus (or the whole SYSRES pack) are
maintained for a period of time, it should be possible to restore
the nucleus from an old back-up.  However, the longer that the
error goes undetected, the greater the likelihood that the
corrupted nucleus will have 'filtered' through to all the back-ups
currently maintained.  If no IPLable nucleus can be restored,
the error can only be cured by IPLing a test system and re-
genning the production system nucleus from it, or, if no test
system is available to do this, the VM starter system may have
to be re-installed!

It is highly unlikely that such a catastrophe could occur, but
there is a very simple way to protect yourself from the
possibility of it happening to you, without having to
intentionally over-allocate the $CP-NUC$ space.  If you place
the $CP-NUC$ area at the very end of the SYSRES pack, even
though the generation program will not issue a message when it
over spills the $CP-NUC$ area, it will issue a message if it hits
the end of the physical pack.  Once you receive the message,
steps can be taken before the next VM IPL to correct the
problem, either by restoring the old nucleus or creating a larger
$CP-NUC$ area.

MAINT 190  and 19E mini-disks

The 190 and 19E mini-disks of MAINT are usually quite
heavily accessed in proportion to other CMS disks on the
system.  This would especially be the case for the 190 disk, if
the CMS system has not been saved in an NSS, and users are
IPLing CMS from the 190 instead of from the saved system.
In either case, it is recommended that these two disks are
placed towards the centre of the pack on which they are
defined.

MAINT 19D mini-disk

The 19D disk holds the files which make up the CMS Help
facility and so, depending on the usage of this facility, it may
be necessary to place this disk towards the centre of the pack.
In most cases, however, performance requirements and usage
are quite low for this disk and so it is recommended that it is
placed towards the outside of the pack.

Other heavily used mini-disks

Before doing a reorganizing exercise it would be desirable to
identify which other CMS mini-disks are heavily used within
the system.  Obtaining this information is made simpler if you
have a VM performance monitor installed which can provide
you with I/O counts for all mini-disks defined to the system.

A heavily used disk is likely to be one that is owned by a
virtual machine that constantly writes information out to disk
(perhaps to maintain a log of activity) or perhaps a CMS batch
machine which handles processing for a large number of CMS
users and uses a mini-disk for some sort of work area.  It is not
likely that disks owned by 'normal' CMS users will fit into this
category.

Obviously place any disks identified as having a particularly
high I/O count as near to the centre of the pack as possible.

MAINT source disks etc

Many of MAINT's disks contain source code, fixes, control
files and program modules which are used by the VM systems
programmer to build and maintain a VM system.  None of this
data is accessed by the VM system itself during VM operation.
The program modules are generated into a nucleus which is
read into storage at IPL time and portions of which are then
paged in and out under normal paging.

In fact, the only time these disks are accessed is while
maintenance (such as applying fixes or whole PUTs) is being
done to the system.  It is highly recommended that these disks
are placed towards the very outsides of the packs and
preferably on packs that contain PAGE areas.  This will have
the effect of fully utilizing the disk space on paging volumes,
without causing an increase in I/O to those packs.

OEM source disks

The observations made above regarding MAINT's source disks
also apply to those disks owned by any OEM products that
have been installed on the VM system.  Only those disks that
are regularly accessed should be placed towards the centre of
the pack and any others which contain source code etc should
be placed towards the very outside of the packs on which they
reside.

TDSK areas

The TDSK areas defined to the system ($T-DISK$) are used by
CMS users to temporarily define mini-disks for the duration of
a CMS session.  Temporary disks are released when the user
logs off CMS or explicitly issues a command to release them.
The released space is then placed back in the TDSK pool.

The level of usage of the TDSK areas will heavily influence
their recommended positioning.  In most cases, unless they are
very heavily used or optimum performance is required when
using them, it is recommended that they are placed towards the
ends of the packs on which they are defined.

CMS user mini-disks allocated on new VM volumes

We have already discussed where existing CMS users' mini-
disks should be placed, but we should also consider where any
new allocations of mini-disk space should be made -
particularly with regard to allocations on new volumes that are
given to the VM system after the reorganization has been
completed.

Often when a new pack is provided for VM/CMS use, a PAGE
or SPOOL area is defined in the centre of the pack (probably to
keep up with demand), and then any new CMS user mini-disks
that are created are allocated from cylinder 1 inwards.  This has
the effect of immediately causing the maximum head
movement required to access the allocated areas on the pack, to
increase to half the length of the pack.  It is recommended that
if a PAGE/SPOOL area is placed in the centre of a new pack,
any new CMS mini-disks that are allocated are placed around
the centre of the pack surrounding the PAGE/SPOOL areas.
Further allocations should be made from the centre outwards,
so that the maximum head movement is required only when the
disk is full.

$ALLOC$ record

When a VM pack is formatted and the various types of DASD
space are defined (PAGE/SPOOL/TEMP/DRCT/PERM etc) an
ALLOC record is created in the first cylinder of the pack.  This
record has details of the layout of the pack and where the areas
have been defined.  This information is required by the VM
system to inform it where the various types of area (PAGE,
SPOOL etc) have been defined.  When planning the layout of
the packs, there is no need to concern yourself with thoughts of
any I/O that the system might do to the ALLOC record on each
pack.  It is only read at IPL time (and also if the pack is
VARYd OFFLINE/ONLINE) and the information contained in
the record is stored in VM control blocks.  Therefore any
information required by the system regarding the layout of the
packs is obtained from the control blocks and not from any I/O
to the ALLOC records themselves.

Monitoring the results

After spending a large amount of time on a reorganization
exercise, it is usually desirable to be able to see that the
changes made have been beneficial!

To effectively carry out a reorganization exercise you need to
have some sort of VM performance monitor/tool installed.
Without one, it would require a large amount of 'site-specific'
knowledge to be able to correctly predict the level of I/O to
certain areas and mini-disks within the VM system.

An effective performance monitor will be able to give accurate
I/O counts to disks and also other information that will not only
help to plan the reorganization but also prove that the changes
made have improved the performance of the system.  It is
therefore essential that, before the reorganization is started, you
begin to collect all the performance related information that
will enable you to make the correct decisions for the placing of
the VM spaces and also that will hopefully begin to show the
results of the changes in the form of before/after comparisons.

Here are a few areas of performance information that should be
useful to monitor:

%   Balance of I/O across devices/channels

%   Maximum/minimum queue depths to devices

%   Average DASD SEEK length/time per device and whole
system

%   Average time CMS user in WAITIO status.

This article will be concluded next month when we shall
publish the author's ideas about a 'system-only' SYSRES pack.


Phil Driver (UK)   ) Xephon 1990


