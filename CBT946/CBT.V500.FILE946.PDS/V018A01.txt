The limitations of SQL/DS
This article addresses concerns about SQL/DS - the IBM relational database that
runs under VM/CMS.  None of the topics discussed below applies to DB2.
ADMINISTRATION LIMITATIONS
My observation is that administration of an SQL/DS database becomes a problem as
soon as more than one user is running against the database, and increases with
the number of users and volume of data.
One of the first requirements for production operation of SQL/DS is the
accounting for resources.  Most large shops like to be able to charge back for
resources and attempt to formulate a capacity plan for these resources.  The
tangibles that can be accounted for are  CPU usage and disk space usage.
Accounting records made by SQL/DS can measure the amount of processor usage by
user-id from connect to release, and the amount used by the database.  However,
there is no accurate way to record the amount of disk space consumed by a table.
Having accounting information could provide growth and charge-back information.
The SHOW DBSPACE command will return the number of 4K database pages consumed by
a database space.  Information in the system catalog provides approximate values
for table sizes which are used for access selection and cannot be used to obtain
reliable sizing information.  No information about the size of indexes is stored
in the catalog.  This is perhaps more important than data about that table
itself because of the ease with which indexes can be created.
ÝEditor's note: some help in this area was given in an article in issue 7, pages
36-39.¨
There are a number of operator commands that can be executed to obtain
performance information.  These can be executed from a program so that
rudimentary monitoring can be done.  The commands return character string data
that is difficult to decipher programmatically and is more useful qualitatively
than quantitatively.  Understanding what the information really means requires
much study of the manuals and never seems to answer the questions you really
want to ask as an Administrator, such as, "Which user's program statement is
causing another user to be locked out?", "What is the database processing now?"
and "How much processor time was used by the database processing the last user's
query?".
The interactive tools need to be much better to provide timely, accurate answers
to the commonly asked questions.  These sorts of tool are available to monitor
operating systems.  The same things are required for SQL/DS.
There are perhaps two reasons why such tools have not been introduced.  The
first is the difficulty of obtaining information about the internal structure
and operation of the database.  The product is one of the first IBM object code
only products for VM/CMS.  The newer licensed material manuals, that are
supposed to make up for the lack of source code, ironically contain less
information than they did when the product was first introduced and are worse
than manuals for which source code is available.
The other reason is the difficulty of getting the database to respond to queries
while it is processing a long-running task.  The SQL/DS scheduler is non-pre-
emptive, which means that, when a long running operation is executing, the
database lets the operation run to completion before looking for new work.  The
scheduler does not 'come up for air' in the middle of such operations to see if
there is other work that can be processed.  The effect is to respond to operator
query commands after interesting events have happened.  This could be remedied
by altering the scheduler so that it time slices.  In the meantime, monitoring
has to be undertaken by using post-processing information, which implies
examination of the trace table.
Much valuable data is available in the trace table.  Areas of the trace table
are labelled with numbers called trace points.  Each trace point has a
significance, eg 4014 traces all the RDS dynamic statement executions.  There is
a supplied program that formats some of the data for problem diagnosis.  Lack of
documentation on the format and appearance in the trace table compounds the
problem of making real use of it to help you administer the database.
The trace table is written to disk on the database virtual machine at the
request of an operator command.  There are some nice things in this data; there
is an audit trail of executed commands and modules.  The data is very useful for
monitoring and debugging.
The trace is written to a file which limits its use to after-the-fact
examination.  If it were directly available it could be used more productively
to take action at the time of the event.
 The trace lacks some important information in its audit trail.  It does not
provide the SQL statement number within an access module that is being executed.
If this were provided, more accurate analysis of performance and contention
problems could be made.
Assuming enough timely information is available to attempt performance tuning,
feeding it back into the database becomes an issue.
At present the parameters that control the scheduler bias, the buffer pools
sizes etc, can only be set at the time the database is initialised.  They cannot
be dynamically altered during execution.
This is also true for the way in which disk space is assigned for use by SQL/DS.
To add new 'slots' to a storage pool which can later be occupied by database
spaces, or to add a new extent to an existing pool, the database has to be
brought down and an EXEC procedure run.  Ideally the reconfiguration should be
possible by executing operator commands while the database is still executing.
Therefore, in order to change the database, it has to be terminated,
reconfigured, and restarted.  This is unsatisfactory if you are trying to
maintain continuous operation.
Relational databases conceptually provide a flexible data structure that can be
used to model any environment.  As the environment changes so can the data model
be changed to reflect the environment changes.  Unfortunately SQL/DS does not
have the concept of flexibility in its operation.  There are many restrictions
upon the manipulation of objects that hamper taking a flexible approach to using
SQL/DS.
One the of first restrictions is the inability to rename objects.  It is
impossible to rename spaces, tables or views.  This capability would make it
possible to substitute objects in the database perhaps to facilitate flip-
flopping between a test and production environment.  It is possible to
circumvent the problem partially by creating synonyms for tables and views.
However, synonyms cannot be set globally.  Each user has to set his own synonyms
for the object individually.  This detracts from the power of the database
administrator, who cannot act on behalf of a user.
Normalising tables is another problem.  When you start building applications,
tables are created that support only the specific applications.  As the scope of
applications grows generally the number of columns and of tables grows.  Columns
can be added to a table quite simply by using the ALTER command.
There comes a point where it becomes necessary to split tables up to obviate the
need for repeating fields and to normalise the data.  The procedure would be to
create a second table to contain the repeating elements, populate it from the
original table and then delete columns from the original table that are now in
the new table.  There is no command to delete columns from the original table -
the ALTER statement only works in one direction.
It is similarly impossible to rename columns using the ALTER statement.  This
might be a useful capability if a naming convention was introduced after the
table was populated.
In order to normalise a table it is necessary to depopulate the original table
completely by unloading it to a disk file, dropping the table, recreating the
new tables and then reloading them from disk.  This is a lengthy and time
consuming procedure.
When the table is dropped after it has been unloaded any objects that depend
upon the table are also dropped by SQL/DS to maintain self consistency.
Therefore prior to dropping the table the definition of all objects that depend
upon the table, such as indexes, views etc, must be saved as well.
If you use the database unload/reload utility (SQLDBSU) to unload the table the
utility attempts to maintain integrity of the table by storing the names of the
table columns that are contained in the unloaded file.  If you attempt to use
this file to reload the new tables the procedure will fail.  The reload utility
compares the column names in your new table with the names it has stored.
A similar problem arises when you attempt to migrate an application from one
database to another  (perhaps from test to production).  The unload/reload
utility is capable of moving data from one database to another, assuming the
column names are the same.  You still have to provide a method for unloading
table dependencies because they are not captured by the utility.  It is also
necessary to transfer program access modules that are used to access tables on
behalf of application programs.
It is difficult to ensure that all access modules are moved.  The database
catalogs maintain relationships between programs and tables that are accessed
 statically.  Programs that perform dynamic statement processing will not have
any recorded relationship with the tables they access.
Access modules cannot be removed from the database by using the unload/reload
utility.  Therefore they have to be recreated in the new database by pre-
processing.  For reasons stated later this is a time-consuming and resource-
intensive activity.
If you fill up a storage pool the database abends.  You have to monitor the pool
usage periodically to make sure this does not happen.  This obviates a hands-off
approach to operations without giving every user his own database machine.  This
situation is much easier to encounter than you might expect because of indexing.
If you have a situation where your users each have private DBSPACES, and  you
over-commit the pool (to maximise disk utilisation - this is possible because
space is not taken until it is actually used).
The private space users can create tables in their spaces and set indexes on
columns to access data.  If an unknowledgeable user creates a large number of
indexes on his tables and populates the table, the table becomes inverted,
effectively creating a very large amount of index data for a relatively small
amount of usage data, hence filling up the shared storage pool and bringing down
the database.  The user in question should be made responsible by warnings from
the CREATE INDEX statement and the database should not fail.  Perhaps there
should be some limits set by the DBA to the number of columns/tables/indexes
that a user can create.
Archival is slow and locks out all users while it is taking place.  Things were
done to remove dependence upon it by providing a UARCHIVE command at Release 3.5
so that the database can be made consistent before using Data Dump Restore (DDR)
or some other device-dependent back-up program.
To speed things up you have the option just to archive the log and only provide
log recovery.  The new release provides bigger tape block sizes but this will
not completely answer the problem.  Archival should be possible by table or
DBSPACE so that it would confine the impact to fewer users.
The tape support in SQL/DS suffers partly at the hands of VM tape handling and
because there is no way to interface suitably to a tape management system.  This
whole area is particularly weak when archives need large numbers of tapes.
Various methods have been devised to get around the problem, eg using
Programmable Operator (PROP), but some better method of automatically requesting
tapes from an automated tape management system needs to be supplied.
APPLICATION LIMITATIONS
When you get around to designing new applications to run on SQL/DS there are a
couple of limitations that you should be aware of that limit the success you can
have in providing an application that functions the way it was designed to.  The
best way to illustrate these flaws is to describe two possible applications that
are difficult to implement.
SQL/DS provides VM/CMS with file sharing.  Having this capability opens up the
possibility of implementing a number of applications that were difficult or not
worth doing before.  One such is an electronic mail application.
By having a shared file system multiple message copies need not be sent to every
individual user through the spool.  In current systems (such as PROFS) this
causes enormous spool management problems, and can significantly degrade system
performance.
An electronic mail system could be implemented by having two tables in the
database.  The first one must hold message text.  Messages in this table need to
be identified by a unique identifier and perhaps have creation date and creator
associated with them.
The second table is a distribution list.  This should contain the foreign key of
the message text, the names of user-ids to receive the text, the date on which
they each read the message, and the date they purged the message from their 'in
tray'; there should also be a way in the distribution list to indicate that the
person in the list received a blind carbon copy (BCC) of the text - this is
probably a flag.
A person receiving a BCC of a message can see the names of all other people on
the distribution list, but all others on the list are not aware of the person
receiving the BCC.
A problem arises when you consider how to permit the distribution list to be
seen by the names on the list exclusive of the BCC names (the CC list), and the
whole list to be read by the BCC names.  The answer is obviously to use VIEWs.
The application would insert the text data into the text table, write the
 distribution list into that table, then create a view of the text and two
separate views of the distribution list, one to span the whole distribution list
(for BCC) and another to span the distribution list excluding the BCC names.
To read mail users would be sent the view name (perhaps in the reader) to permit
them to read their mail.  The volume of data would be smaller than that needed
to send everyone separate copies.
There is a security hole in this application.  The people on the CC list can
still find out who is on the BCC list.  It can be found out by simply
correlating SYSTEM.SYSTABAUTH with SYSTEM.SYSVIEWs.  The view will be a select
statement that searches for the presence or absence of the BCC flag being set in
the distribution list for a particular message.  For a user to find who has been
blind carbon copied the view can be searched for with the flag set for the
specific message, using the SQL 'LIKE' operator.  The result can be traced to
the authority table to see who has the permission to use such a view.  Hence it
is possible to determine the BCC list.
The way to stop this is obviously to restrict SELECT access to the views but
still allow users to execute the views.  This has to be something like the run
permission that can be granted to access modules.
The following points relate to storing textual information in the database.
Text is any long string of character information.  This could be a long
document, a graphic image, the plans to a building etc.  SQL/DS is not a
suitable place to store data of this sort, basically because the relational data
model itself does not address itself well to this type of data.
A column datatype called LONG VARCHAR is provided to store text data.  Strings
stored in columns of this type have to be no greater than 32 767 characters
long.  This constraint requires an application program to always ensure that
data destined for storage must fit in the field and, if it does not, to slice it
up and place it somewhere else.  Similarly, outgoing data must be restrung
before use.
Indexes can not be placed upon LONG VARCHAR columns so powerful SQL search
capabilities cannot be used to find specific textual references.
The SQL interactive utility program (ISQL) that permits terminal access to the
database is incapable of displaying columns of this datatype on a terminal
screen.  So if you want to visually inspect or modify such data you have to
resort to other means.
To circumvent these restrictions designers usually end up arbitrarily splitting
and storing text data in fixed length strings stored in table rows that have an
adjacent sequence number column.
The length restriction should be removed and some method provided to establish
indexes on arbitrary strings to provide fast text search capabilities.
An additional capability could be provided by the database to extend write locks
across sessions so that library systems could be implemented easily.
The original design for SQL encompassed a scheme for specifying referential and
data validity checking to the database so that consistency could be maintained.
This would be a nice feature if it were possible to implement while still
maintaining a reasonable level of performance.
At present, I do not believe the lack of this capability to be a serious
limitation.  Any additional features that further impair performance would be
detrimental to the acceptance of the product.  More effort has to be expended by
IBM in simply making the database perform adequately as implemented.
The response time to a query has to be improved, and the number of concurrent
processes has to be greater.  Each impacts the other.  If transactions can be
processed faster, contention can be resolved quicker.  If there is less
contention response will be faster.
APPLICATION PROGRAMMING LIMITATIONS
When you finally get down to coding applications that run against SQL/DS you
encounter another set of limitations that need to be resolved.  Some can be
circumvented if you are aware of them beforehand.  I hope that the following
list will help.
When you connect to SQL/DS you establish an LUW.  At the point you issue a
COMMIT WORK or a ROLLBACK the current LUW is ended and another one started.
There is no means to start one LUW, then leave that one running and start
another one (perhaps connected as a different user-id)  by issuing another
CONNECT to the database.
In practice there should be no difficulty in doing this, because:
1    The database is multi-tasking.
 2    The Inter User Communication Vehicle (IUCV) connection is multi-pathed.
Your own virtual machine does not necessarily need to be multi-tasking (which is
difficult to do if you are running CMS).
The IBM SQL language is part of the problem.  There is no language keyword to
associate an SQL statement in a program with an LUW.  The ANSI committee that is
reviewing standards for SQL has made suggestions for a statement to set a
transaction identifier to provide for this capability.
Why would you want this capability?  Let us say you have an ability to multi-
task CMS P not unreasonable considering that features in Release 5 of CMS permit
many terminals to be connected to a virtual machine.  This makes it
theoretically possible for many users to each execute the same application
(assuming re-entrancy).  However, if the application is responding to the needs
of multiple users then each user has to have an LUW in the database.  Having
only one LUW available at the time within the virtual machine makes this
impossible.
Problems arise when working with more than one database at a time.  The
databases in question can be on the same or different processors (quite feasible
as of Version 2); the limitation is still the same.  It should be possible to
dynamically access different databases simultaneously and UNION tables from
both.  At present one database can be accessed at a time, the switch being made
at the end of an LUW.  Work can only be done with one DBMS at a time.
Therefore, to join data from two databases the program must extract data from
one database buffer, point to the other database and extract the data from it,
and procedurally join the data.
When a call is made to SQL/DS to process a command, the call has to complete
within the database machine and a response returned to the calling program
before execution can continue within the program.  In a single programming
environment like CMS this is an acceptable mode of operation.  It guarantees
that a program does not continue to any further processing step until the
success or failure of a command has been determined and the data sent to the
database to be stored has, in fact, been stored.  It forms the basis of a sound
contract for a program doing business with SQL/DS.
However, if the environment that is transacting with SQL/DS is a multi-
processing environment that is perhaps dealing with more than one user/program
at a time that is it is impractical to stop the processing of the whole virtual
machine to wait for a reply from the database that is destined only for one user
when another user could be scheduled to do useful work.  This is only the same
age-old principle that has been employed by multi-tasking operating systems for
a long time.
Providing an asynchronous interface would permit database commands to be issued
and further processing done, the result of a command being returned to the
application as an IUCV interrupt.  The interrupt would be processed and work
resumed.  This capability would work in combination with the ability to process
multiple LUWs from within the same program.
To write a program to read data from a table in a program you code a SELECT
statement and declare a cursor to point at the selected rows in turn.  The
initial cursor position is established by opening the cursor, successive rows
are read, and the cursor is closed when the set is finished with.
If the LUW is ended while the cursor is still open the cursor position within
the set is lost, and the cursor becomes undeclared.  Ideally, the cursor
position should be retained, and not become undeclared.  This is very important
for keeping database lock contention to a minimum during long-running
applications.  Locks are released at the end of an LUW.  If an LUW can be ended
frequently, theoretically, the number of users working simultaneously can
increase.
Long-running applications are not necessarily batch applications.  An
interactive editor can cause such problems.  If the editor reads rows with an
intention to update them, a user scanning down a set can lock out the work of
others.
Cursors are tied to a query for the duration of an LUW.  If the association
could be removed without the necessity of committing the LUW, more economical
use could be made of them.  This is particularly acute for programs that attempt
to dynamically read SQL statements from a file or terminal without any pre-
emptive knowledge of the command, and then execute them against the database.
These types of program have to manage their use of cursors from a large pool
that may not even be used.  There is also the risk that if the application is
 coded so as not to use parameterised statements the number of cursors will be
quickly used up.  This results in large virtual memory usage in the database and
user virtual machine and can cause the application to terminate through lack of
cursors or memory.
To read data from a table dynamically it is necessary to request the column
lengths and data types that are going to be returned.  The cursor is declared
and opened and successive rows read into the program.
The cursor is moved forward to the next row after each successive read.  There
is no way of positioning the cursor backwards over rows that have already been
read.  There are many types of application where being able to scroll back over
previous data values is a strong requirement. A good example is an editor
application or one in which rows are displayed one per screen.  It is a
frustrating experience for a user to be able only to move forward through a set
and not to be able to backtrack through it.
In order to be able to provide this facility the application that is front-
ending the database must buffer rows either in memory or on disk and simulate
scrolling on the database.  This buffering approach affects the accuracy of the
data, ie data may be changed in the database while being held as an extract.
This external necessity for the application to buffer data places additional
overhead on machine resources by additional paging, and CPU load for free
storage management, and I/O for disk reads/writes.  The database machine itself
could provide the function since it has the data available from the initial
request, although it places the same demand on the database machine that is
placed on each application virtual machine.  There should be an extra facility
to request scrolling cursors at the beginning of the call if needed so that the
database buffers only specific data.
To obtain the number of rows that will be returned from a query it is necessary
to issue a separate SELECT command that has a COUNT function within the command.
This necessitates that the application program generate an additional COUNT
statement by reformatting the SELECT into a SELECT with a COUNT function.  This
involves some parsing and string handling, especially if the program is
dynamically processing queries.  This is extra work for the programmer and
doubles the database processing of the query: once for the row count and once
for the acutal query.
Queries are processed as stated above by passing SQL/DS the SELECT statement,
asking for the format of the data returned, associating a cursor with the data,
and then opening the cursor on the query.  At the point the cursor is open it is
expected that SQL/DS will know the number of rows that will be returned in the
query.  This should be more true if the SELECT has requested an ORDERing of the
data that necessitated a SORT be performed.
The row count should be made available after the OPEN without the need to issue
a separate command.  There is a field in the SQLCA (the control block returned
by SQL/DS after a call) that is called the row count.  However, this field does
not contain a value after the OPEN.
Another separate problem exists in determining a row count.  This results from
where the count function is executed within a query.  It is not possible to
return a count of the number of rows from a UNION.  The syntax only permits the
UNION of the count to be returned, eg to return a count from a SELECT you code:
     SELECT COUNT(*) FROM ...
To union two queries you code:
     SELECT co1 FROM ...
     UNION
     SELECT co1 FROM ...
To union the counts of two queries you code:
     SELECT COUNT(*) FROM ...
     UNION
     SELECT COUNT (*) FROM ...
The only strategy to obtain a count of the number of rows returned by the query
is to run a count for both SELECTs and take the larger number.  The result can
be in excess of the value you want to obtain.  For applications that depend upon
this value this could result in an over commitment of resources.
What is required is a function that acts upon the result of the union, eg:
     COUNT(
     SELECT co1 FROM ...
     UNION
     SELECT co1 FROM ... )
The restriction on views does not allow a view that spans more than one table to
be updated as if it were a table.
Views are therefore only useful as a way of protecting queried data and to
provide query independence from database structural changes.  This limitation
forces you into coding update applications that are dependent upon database
structure.  If you alter the structure of the tables, perhaps by normalisation,
programs need to change to reflect that change.  This could be a big problem if
you have many programs that depend upon the structure.
This restriction also affects the administration of the database in ways similar
to those mentioned at the start of this article.
The pre-processor is a utility supplied with SQL/DS through which programs must
be passed before they can be run against the database.  It has two functions:
1    It creates an Access Module in the database
2    It turns SQL into calls.
There are a number of subsidiary problems with working with the pre-processor.
Since it writes an access module into the database it places a write lock on the
system catalog tables.  This effectively stops any other pre-processor work
being done and stops any data manipulation commands like CREATE TABLE being
processed until it has completed.  The database is effectively single-threaded.
This whole approach leads to programmer frustration through having to wait in
line for pre-processing.  If you had a reasonable-sized group of programmers
working on development, all of whom were attempting to use the pre-processor,
work could build up into a back-log.
The pre-processor is a simple compiler.  Its execution time is to some extent
proportional to the length of program being processed.  A sensible development
approach attempts to remove program development from the dependence on pre-
processing.  This can be done by coding modular programs that have their
database I/O confined to separate modules.  Every program that wants database
access can dynamically call the I/O modules for service.  The application
development process can proceed by coding only the functions above the database.
However, at present a large penalty is paid in the execution of security
checking code in the database whenever access modules are called dynamically.
This is supposed to have been changed with Version 2 Release 1.
The other failing of the pre-processor is the amount of redundant code it
produces within an application program.  Every SQL statement causes the pre-
compiler to create a fully initialised control block in line in the code.  This
can significantly increase the size of a program to the point where the program
is too big to be loaded in the available memory.  This can happen if the program
is being used co-residently with products loaded in Discontiguous Shared
Segments (DCSS) low down in the address space.
Ideally the pre-processor should place a call in the source code that creates a
single control block in free storage at the start of the program.  The control
block could then be completed by data written into it from calls embedded in the
program written there by the pre-processor.  Calls to the database would be made
by passing the address of this control block created in free storage, not the
different address of the different control blocks that are created in-line in
the user storage of the program.
 The capability of dynamically processing an SQL statement against the database
at execution time without previously pre-processing the specific command is a
valuable feature of SQL/DS.  This capability makes it possible to make use of
the tremendous power of SQL/DS to process ad hoc queries from within application
programs.
Unfortunately, because of the way this interface is written, its power is
confined to Assembler or PL/I programs, neither of which is a Systems
Application Architecture (SAA) language.  The interface is not supported by the
pre-processor from either COBOL or FORTRAN.
The key to this interface is the manipulation of free storage in the virtual
machine, and the ability to process address variables, neither of which exist
within COBOL and FORTRAN.  The interface is also easier to use if the host
programming language can easily manipulate strings of characters since the SQL
language is simply a wordy string.
If the free storage and address manipulation were removed from the host language
into support routines, calls to which were embedded in the program by the pre-
processor, some support for this interface could be provided.  Obviously, since
the typing of data is left until run-time, the host language has to have enough

 pre-defined variables set aside to take care of any data that might be returned
into the program.
Considering the last point it seems an absolute necessity to provide a 'C' pre-
processor, since 'C' is an SAA language that does not suffer from the
shortcomings of COBOL and FORTRAN where storage and address variables are
concerned.
SQL is an extremely wordy language.  To formulate a query it is necessary to
construct a long character string.  This requires memory within the program and
good string manipulation routines if the queries are to be formulated
dynamically.  It also mandates having an SQL parser of some sort embedded in
your application.  If the language could be abbreviated to shorter strings much
effort could be saved, say if 'SELECT' could be shortened to 'SE'.
CONCLUSION
SQL/DS has limitations in three areas: administration, application support, and
programming.  Methods can be devised to work around them before IBM gets around
to fixing them.  In the meantime being aware of them can ensure a more
successful experience of the use of SQL/DS.
A Blakey
Director Customer Service
Kolinar Corporation (USA)     ) Kolinar 1988














































