Two-level VM

Since the mid- to late-80s some VM/MVS sites have been
known to run two different versions of VM concurrently, as
well as the usual production/test MVS system(s).  This article
answers the following three questions:

o      Why have some sites been running two versions of VM
      concurrently?

o      What are the overheads caused by running two VM
      systems and what savings could be made by eliminating
      one of them?

o      What would be the recommended method for migrating
      from a 'two-VM' to a 'one-VM' system?


WHY RUN TWO VERSIONS OF VM CONCURRENTLY?

In a 'two-VM' site, one version of VM is usually VM/XA
which is run at the 'first-level' (ie the operating system which is
IPLd in the real machine).

VM/XA was originally designed as a migration aid to assist
MVS/SP users to convert to MVS/XA.  Initially, VM/XA
Migration Aid Version 1 (and later Version 2) was installed to
run at first level so that MVS/SP sites could run their current
MVS/SP systems under it at the second level.  They could then
install and test their new MVS/XA system also at the second
level.

The second version of VM in a 'two-VM' site is usually a
VM/SP or VM/SP HPO system which is run at second level
under the first level XA system.

The reason for running two types of VM system was because
sites wanted to take advantage of XA architecture by running
MVS/XA, and offer full-function CMS.  To continue to
provide a facility for multiple guest operating systems to be run
on one CPU (without the use of PR/SM or MDF etc), the VM
installed at first level had to be VM/XA in order to support
MVS/XA.

The version of CMS available to run under VM/XA Migration
Aid (and later VM/XA SF) was not a full-function CMS
system.  It was not capable of supporting many of the IBM and
OEM application packages that were available and in use under
existing VM/SP systems.

Sites had to choose either to convert to VM/XA with the
possibility of 'losing' those IBM or OEM application packages
that were not supported by the CMS provided, or stay with
VM/SP and MVS/SP until a VM/XA system was shipped with
a full-function CMS (as was being promised for the future).
The decision was based on how heavily they were using their
VM/SP environment versus how much they needed XA
architecture.

Alternatively they could install VM/XA at first level with the
new MVS/XA system under it at second level.  Then install a
VM/SP system also running second level to support the
IBM/OEM software running under its CMS.  A conceptual
view of this configuration is shown in Figure 1.  Some sites
took this option and so began running two levels of VM.

When VM/XA SP2 was released, it was finally accompanied
by a full-function CMS system which supported the majority of
IBM and OEM application packages and vice versa.  As a
result of this, sites could install one VM system which was
capable of supporting MVS/XA and all applications packages
that they had running under VM/SP - thus eliminating the need
for retaining the second level VM/SP or HPO system.

However, some sites continued to run two VM systems.  This
was probably due to a lack of manpower available at the time
to take on the seemingly large task of migrating all the IBM
and OEM applications (plus all the users of the applications)
from under VM/SP to VM/XA and then removing VM/SP all
together.


OVERHEADS AND SAVINGS

'Overhead' in this context would usually suggest an additional
CPU usage etc; however, the effects of running two VM
systems spreads much further than that.

Other than CPU usage, there are four key areas where the
'overhead' can be reduced (or savings made) when reverting
back to a 'one-VM' system.  To better illustrate these savings, I
have used genuine figures obtained from a site that I spent time
working at, which was running a 'two-VM' system.  The key
areas of savings are shown below.

Disk space savings

By maintaining two versions of VM, vast amounts of disk
space can be wasted by having duplicated CMS
service/maintenance machines and CP system areas etc.

Figure 2 shows the disk space usage from the sample site's
VM/SP system for areas that were duplicated at the VM/XA
level and which would subsequently be freed up when VM/SP
was removed.

Although it is appreciated that some free space would have to
be carried over to the VM/XA system to allow for future
growth (and the figure is particularly high at the sample site), it
is still likely that the total free space across both VM systems
would be more than enough to allow for future growth in the
new VM/XA system.

In the example, the DASD space that would be released when
the VM/SP system was removed was equivalent to almost three
single density 3380s.  This figure is perhaps a little high
because the VM/SP system in question was badly managed,
and had too much free space on its packs.  Even if the last
figure for free space is ignored, there would still be nearly
2000 cylinders of 3380 disk space released by eliminating the
duplication of system areas etc.

Cost savings

There may be cost savings made by eliminating the VM/SP
system.  Firstly, the cost of VM/SP itself will be saved,
assuming it is still being payed for (more than likely).  Also
costs of other duplicated IBM/OEM products such as
performance monitors, back-up packages, multisession
managers etc may be reduced if charging is done for each VM
system they are used on.

Easier operational procedures

Running VM/XA with VM/SP, MVS (and maybe even VSE)
running under it is complex for operators.  They will appreciate
the removal of a superfluous level of operating system.
By removing the VM/SP system, the following benefits could
be expected:

o      One less operator console to have to worry about.

o      Quicker orderly IPL/shut-down of whole system, therefore
      easier training of operations staff.

o      Less ATTACH/DETACHing of devices (particularly tape
      drives, cartridges etc) between the two VM systems.

o      Less time taken to carry out back-ups (unless freed space
      re-used immediately by MVS or VM/XA system).

o      Fewer commands to remember (VM/SP and VM/XA have
      some different operator commands).

Savings in system programmer time

Eliminating VM/SP would also remove the requirement for the
systems programmer(s) to maintain two levels of VM.  No
longer will they have to apply FIXs/PUTs to two different VMs
(using two totally different methods) as well as keeping all
other IBM/OEM software running under them up-to-date.
There will also be only one set of VM manuals to have to
worry about as well!


RECOMMENDED METHOD FOR MIGRATION

We will answer the following three questions:

o      How can the migration be done?

o      What planning is required?

o      What testing can be done to ensure the migration goes smoothly?

How can migration be done?

It is important to carry out the migration with the minimum of
disruption to the VM/XA and VM/SP user bases.  Once the
benefits of migrating have been appreciated, it is important that
the work be carried out as quickly as possible, but without
compromising on full pre-migration testing and also ensuring
that secure backout methods are available should they be
required.

To achieve these goals, it is recommended that the whole
migration be done 'in-one-go' in the form of a big-bang.  This
method is preferable to a gradual migration whereby single
applications/users are migrated over to the VM/XA system in
an effort to finally run-down the VM/SP system.  In my
experience, I have always found the 'running-down' method of
migration (regardless of what it is that's being run down) to be
very susceptible to dragging its feet.

Let's overview how the big-bang will work. We will use the
sample system mentioned earlier as an illustration.  Figure 3
shows the DASD layout of the sample 'two-VM' system before
the migration.

In the example, VM/XA has three packs defined to it and the
VM/SP system has five.  Each system is currently IPLd from
its '..SRES' pack and the users of each VM are defined in the
relevant VM user directory for that system.

Figure 4 shows the first of two stages of DASD layout after the
big-bang has taken place.

In Figure 4, all eight of the VM (XA and SP) packs have come
under the umbrella of the VM/XA system and all VM/SP users
and applications are now part of the VM/XA system in that
they access their mini-disks by logging on to VM/XA.  Notice
that the old VM/SP packs have retained their VOLSERs.

It is fairly simple to achieve this transformation.  Firstly, a
copy of the VM/SP user directory entry for each VM/SP user
(excluding duplicated users in VM/XA and also the VM/SP
system areas) is copied from the VM/SP to VM/XA user
directories.  Remember to keep the MDISK statements the
same so that they continue to point to the VM/SP volumes.
This is why they retain their VOLSERs after coming under
VM/XA control.

Once this is done, the old VM/SP packs can be defined to the
VM/XA system as user volumes (SYSUVOLs).  The XA
system can then be IPLd with the VM/SP disks attached and
the new user directory containing the existing VM/XA users
and the new VM/SP user definitions can be placed on-line.
The old VM/SP users can then log on to VM/XA and access
their old VM/SP mini-disks which are in the same place on the
same volume as they were under VM/SP.

Obviously a lot of unused VM/SP mini-disks will still be on the
old VM/SP volumes, but they will not be referenced in the
VM/XA user directory.  These areas can remain on those ex-
VM/SP packs for as long as you want to provide a simple
backout to the 'two-VM' system.  This backout will be achieved
by shutting down the VM/XA system, re-IPLing without the
VM/SP packs defined, and then re-IPLing the old VM/SP
system from VMSRES with the old VM/SP directory on-line.
Ex-VM/SP users will then be able to log on to VM/SP
and access their disks as they used to.

To protect the backout mechanism, do not use the ex-VM/SP
volumes for defining any new VM/XA areas until you are sure
the backout will not be required.  This will ensure that nothing
gets over-written on the those disks, which is a particular
danger for the ex-VM/SP areas that are not defined in the
VM/XA directory and won't show up as used space on a
DISKMAP.  Alternatively, just define 'dummy' mini-disks over
those areas so they are displayed as allocated space on a
DISKMAP.

Also ensure that any changes to ex-VM/SP user definitions (eg
passwords, mini-disk moves etc) in the VM/XA directory are
reflected in the old VM/SP user directory in case a backout is
required.  To do this you will obviously have to ensure that the
VM/XA MAINT has access to the source VM/SP directory.

Once a reasonable amount of time has passed and it is clear
that a backout will not be required (a maximum of a couple of
weeks should be sufficient), you can then start to release some
of the unused space and possibly free-up some of the packs.

Figure 5 shows the second stage of DASD layout after the big-
bang once a consolidation of free space has taken place.

In Figure 5, two whole packs have been released and the ex-
VM/SP packs have been renamed in line with the site specific
VM/XA volume naming standards so that the two VM systems
have been fully integrated.

What planning is required?

Although some of the points may be site-specific, most of the
areas mentioned are likely to apply to the majority of VM
systems.  Most of the changes can be carried out before the
big-bang takes place.

VM/SP CMS users

To effectively plan for the big-bang, you will need to look at
the users defined to the VM/SP system and judge what effect
moving them under VM/XA will have.  As described in the
previous section, the intention is that the VM/SP volumes will
simply be attached to the VM/XA system and the ex-VM/SP
users will access their mini-disks by logging onto VM/XA
(after being defined in the VM/XA user directory).  This is
unlikely to cause any problems for the majority of normal CMS
users as their mini-disks will be untouched and will remain at
the same addresses on the same volumes as under VM/SP.
There should not be any incompatibility between the VM/SP
CMS and the VM/XA SP CMS with regards to accessing those
disks.

MAINT (VM/SP and VM/XA)

The MAINT user-id (and any other ones set up to do similar
VM/SP specific maintenance work) will become redundant
immediately after the big-bang.  Unless a backout operation is
required, it will not be used again.  For the duration of the
post-bang period when a backout could be needed, ensure that
the VM/XA MAINT has got write access to the VM/SP
MAINT's mini-disk which holds the source for the VM/SP user
directory.  This is so that changes to ex-VM/SP user-ids in the
VM/XA directory can be reflected in the VM/SP source
directory in case the backout is implemented.  Obviously
during this period the VM/SP directory cannot be placed on-
line, but as long as the source is kept up to date, a backout
should be simple to achieve with no loss of changes.

It would also be worthwhile to check for any useful EXECs on
the MAINT VM/SP disks that you may want to carry over to
VM/XA.  Most likely candidates would be EXECs that were
perhaps written to work with some of the IBM/OEM
applications that will also be moved from VM/SP to VM/XA
such as start-up/shut-down or reporting procedures.

Alternatively, the VM/SP system may have a common disk
usually owned by MAINT which holds EXECs that can be
used by all CMS users.  These may also require moving over to
XA if the facilities provided by them are still required.

User directories (VM/SP and VM/XA)

One of the most time-consuming parts of the preparation will
be to copy all of the VM/SP user-ids from the VM/SP user
directory to the VM/XA user directory.  This is best done by
creating a separate TEST directory (eg TEST DIRECT A)
which will not actually be used until the XA/SP combined VM
system is IPLd.  It can't be placed on-line before the IPL
because error messages will be issued for all the ex-VM/SP
user-ids with mini-disks pointing to volumes that don't exist
under XA until after the IPL.  This can of course be
circumvented, but it is far easier to just remember to put the
new directory on-line immediately after the IPL.

There are some statements which are valid in VM/SP
directories which are not supported in VM/XA.  After you have
created the new directory, issue the following command to test
its validity (assuming the newly created test directory is called
TEST DIRECT):

      DIRECTXA TEST DIRECT(EDIT

This will validate the content of the test directory and report
any unsupported statements or syntax errors.  The '(EDIT' will
prevent the DIRECTXA program attempting to place the test
directory on-line in place of the real one after the validation
completes successfully.

Obviously remember to keep the TEST DIRECT and the real
directory synchronized for the duration of the pre-bang testing
so that all changes in the real one are reflected in your test one.
Because of the possible hassle of this, it is best to delay the
creation of the test directory until as late a possible prior to the
first big-bang test IPL (see later).

HCPSYS

HCPSYS has to be altered to contain entries for the ex-VM/SP
volumes in its SYSUVOL statement (see also notes under
SPOOL, PAGE, TDSK etc).  This change can be made before
the first test bang IPL, but the VM/SP volumes will become
ATTACHd to the XA system, so remember to DETACH them
before IPLing the VM/SP system under it.  Better to have two
separate nuclei generated and flip them in the same way as the
USER DIRECT.

EREP, DISKACNT, OPERATNS etc (VM/SP and VM/XA)

The VM/SP versions of EREP, DISKACNT, OPERATNS etc
will also be made redundant after the migration.  Once again
check for any useful EXECs that may need to be carried over
to VM/XA.  After the big-bang, the VM/XA versions of these
users will probably experience an increase in the amount of
data that they are collecting.  Ensure that their mini-disks are
large enough to accommodate this increase.  Operating
procedures with regards to running reports against these
machines will have to be updated (and hopefully become
simplified).

OPERATOR and AUTOLOG (VM/SP and VM/XA)

The VM/SP versions of OPERATOR and AUTOLOG will also
be made redundant. Ensure that any EXECs on their mini-disks
are reviewed to assess their possible use under VM/XA.  An
example of this would be an EXEC that simplifies some
commands to be issued to a VM/SP guest CMS user.  Review
AUTOLOG1's PROFILE EXEC to ensure that any AUTOLOG
statements it contains to start-up other VM/SP CMS machines
are moved over to the VM/XA AUTOLOG1's PROFILE
EXEC.

Also check what PF keys are set up for the VM/SP
OPERATOR and try to include them in the VM/XA
OPERATOR PF key settings so that operators can continue to
issue commands with the same PF keys. This applies
particularly to commands issued for/to machines formerly
running under VM/SP.

RSCS

It is likely that RSCS links will have been set-up between the
two VM systems (and probably any other guest operating
systems as well).  There shouldn't be too much to change here,
except to remove the redundant VM/SP definitions from the
VM/XA RSCS definitions.  Obviously the VM/SP RSCS user-
id will be redundant.  If there were also VSE machines running
under the VM/SP system, the definitions for them will have to
be moved from the VM/SP to the VM/XA RSCS machine (see
also VSE GUEST MACHINE).

You might want to create TEST RSCS definition files and 'flip'
them over at big-bang time.

SPOOL, PAGE, TDSK etc

It is unlikely that the VM/XA system will have enough space
defined for the SPOOL, PAGE, TDSK etc areas to cater for the
inclusion of the VM/SP processing workload.  Estimates will
have to be made on the space requirements for these areas and
if possible get them defined into the VM/XA system ahead of
time.  To ensure that DASD space isn't wasted in the VM/XA
system, avoid simply adding the two figures together (VM/XA
and VM/SP) as this will obviously result in over-allocation of
these areas!  Do not under-allocate them either, as this may
have a serious impact on the VM/XA system - particularly the
paging areas.  If possible, do not allocate any new space on the
ex-VM/SP packs until the backout is no longer required and
you are ready to go to stage 2 as per Figure 5.  Especially don't
use the already defined VM/SP PAGE, SPOOL, and TDSK
areas for the equivalent VM/XA functions.  If the ex-VM/SP
packs are attached to the VM/XA system as user volumes (ie
SYSUVOL and not SYSCPVOL in HCPSYS) this will ensure
that VM/XA will not use the ex-VM/SP packs for paging or
spooling purposes.

Other IBM/OEM application packages

Each VM/SP site will be running its own mix of other
IBM/OEM application packages and it is impossible to cover
the implications of moving them from VM/SP to VM/XA.
Suffice to say that you should ensure the equivalent package is
installed on the VM/XA system and fully tested.

Any tailoring/customizations should be copied over from the
VM/SP version, so that, at the time of the big-bang, the CMS
users of the application(s) will immediately start to use the XA
version and the SP one will be redundant. This should be fairly
straightforward as most packages have only a few
control/definition files that require tailoring and the rest are
just executable programs.  Take care to keep the two versions of
the application(s) synchronized for the duration of the pre-big-
bang and post-big-bang testing.

One thing to watch out for is that the ex-VM/SP users will
probably have LINK statements for some of the application
disks in their VM directory entry.  Ensure that the addresses
and VOLSERs of these LINK statements are changed to reflect
the XA version of the application.

If users have links set up in their PROFILEs or other EXECs
this may be more of a headache especially if you have changed
the name of the application's user-id. The LINKs within the
EXECs will fail due to invalid user-id.

Duplicate users

Duplicate users are usually non-service CMS users who have
user-ids at both levels of VM, such as systems programmers
and anyone else who would be likely to require access to both
levels of VM system.  Their VM/SP user-id will no longer be
required, so ensure that any useful EXECs and other files are
moved over to their VM/XA machine (which may need to have
its mini-disks enlarged).  This move can be done before the
big-bang unless the user still requires access to any VM/SP
applications which won't be available under VM/XA until after
the big-bang.

VSE guest machine (under VM/SP)

It's possible that your site is still running VSE as well as MVS.
As already mentioned, the sample site had two VSE machine
(PROD and TEST) running under the VM/SP system.

On the whole, guest machines that were running under VM/SP
should be unaffected by the move to running under VM/XA.
Their directory entries will of course be copied over with all
the other VM/SP users and will need little changing.  Look out
for DEDICATE statements (particularly operator consoles) as
it's likely that the virtual address assigned to the device under
VM/SP will not be the same as the real address with which
they should be referenced under VM/XA.  As long as the
actual VSE packs are made available to the VM/XA system
(via HCPSYS) they should be picked up OK.

Any EXECs carried over from VM/SP to do with VSE
machines (for example 'SUBMIT' EXECs) should work OK
under VM/XA as long as the VSE machines are given the same
user-id as under VM/SP.

Any options set in VSE itself to route output back to VM/SP
will have to be changed to allow the output to now go to
VM/XA.  For example, jobs that use 'route' information are
likely to use the RSCS node name for routed output and will
therefore have to be changed to specify the VM/XA RSCS
node name (see also RSCS).  The 'ROUTER' machine may
need changing as well.

What testing can be done to ensure the big-bang goes smoothly?

It is highly unlikely that the big-bang would go 100% right the
first time, so you will probably want to have one or more trial
runs before the real thing.  The idea of a small-bang would be
to IPL the VM/XA system with the VM/SP users all defined to
it (putting TEST DIRECT on-line); IPL the MVS/VSE guests
under it and then run as many tests as possible to ensure that
everything looks OK.  After the tests, return to the old system
until another test is required or the big-bang itself.

The purpose of this section is to give an idea of a running order
for a test including how to get back to the old system.  This
order is not meant to be a comprehensive list of things to do,
but rather an outline of the sort of order in which things should
happen.  It is recommended that a checklist be created to help
you through the test, both as useful documentation and as a
source of guidance for a backout should it be required.

It is unlikely that any 'damage' will be done to the VM/XA,
VM/SP, MVS, or VSE systems, or the users of those systems,
by doing small-bangs, but it is still highly recommended that
you full-volume back-up everything before a test, if this is at
all possible.

1      Move all users who have user-ids at both VM/SP and
      VM/XA levels over to VM/XA unless they require the use
      of an application under VM/SP that won't be available
      under VM/XA until after the big-bang.

2      Test all applications running under VM/SP for
      compatibility with VM/XA.  If an XA version is required,
      install it into the XA system and test.

3      Make any appropriate changes to OPERATOR, EREP,
      RSCS, DISKACNT, and AUTOLOG1 for VM/XA.
      Hopefully no further changes will be made to VM/SP
      versions before big-bang, but if they are, ensure the
      changes are reflected in the VM/XA versions where
      applicable.

4      Create the test directory.

5      Copy common EXECs etc over from VM/SP.

6      Create a new test HCPSYS and include VM/SP volumes in
      the SYSUVOL statement.

7      Back-up current VM/XA nucleus to tape with DDR.  This
      will provide a nice quick backout to the old system via a
      stand-alone restore (see step 17).

8      Full pack back-up all systems if possible.

9      Re-gen VM/XA CP nucleus using HCPSYS created in step 6.

10     Back-up XA spool files (cold start will be done later).

11     Re-IPL VM/XA (stop AUTOLOG1 IPLing anything).

12     Place test directory on-line.

13     Manually IPL MVS/VSE guests and start up any
      application machines.  In fact now the new directory is on-
      line, assuming AUTOLOG1's PROFILE has been properly
      tailored, you can simply autolog AUTOLOG1.

14     You can now carry out all the testing you want to do.  If
      MVS/VSE machines come up OK there shouldn't be many
      problems there.  The biggest area of testing will be to
      ensure that ex-VM/SP users can log-on and access
      applications OK.  You know the applications work because
      you've tested them as part of step 2.

After the testing has been done, you will want to quickly return
to the old system.  The following steps detail how to do that.
These steps will also be the backout path if one is required after
the big-bang itself.

15     Put old directory back on-line and swap AUTOLOG1's profile.

16     Shut down all systems including VM/XA.

17      Stand-alone restore the VM/XA nucleus backed up in step 7.

18     Re-IPL VM/XA (cold). AUTOLOG1 should IPL VM/SP
      with VSE under it, as before.

19     Restore VM/XA spool files backed up in step 10 which
      would have been lost after the cold start.

The system should now be back to normal.  You may want to
do a few more small-bangs, depending on the number of
problems encountered.  You can simply start at step 7 for
subsequent bangs unless major changes have been made or a
large amount of time has elapsed since the last test.  The same
goes for the big-bang itself; again you will probably only need
to steps 7 - 13.  It is recommended that as little time as possible
is left between the first small-bang and the real big-bang to
reduce the chance of errors due to not keeping real/test files
synchronized.

Once the big-bang has taken place, the backout mechanism will
be as described earlier - roughly steps 15 to 19 above.  Once it
is decided that the backout won't be required you can begin to
consolidate the free space with the aim of fully integrating the
VM/SP and VM/XA users onto one set of volumes.

Phil Driver
Systems Engineer (UK)

