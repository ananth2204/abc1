Communications options under VM and VSE


INTRODUCTION

One of the main difficulties in a VM/VSE environment is
choosing the options for running an effective network. The
current range of products, both hardware and software, is vast,
and can be further complicated by other manufacturers
supplying hybrid systems.

In addition, the growing importance of Local Area Networks
(LANs) and how they fit in with the mainframe network is a
major consideration.


SNA - AN OVERVIEW

Systems Network Architecture is the description of the logical
structure, formats, protocols, and operational sequences for
transmitting information units through, and controlling the
configuration and operation of, networks. A true SNA network
would include components such as VTAM, SDLC protocols,
and associated hardware and software products. However IBM
has allowed interfaces to non-SNA items, such as X.25,
Ethernet LANs, TCP/IP, and so on.

Therefore the term SNA is used in various contexts. In most
cases, we tend to define an SNA network as a network that is
controlled by VTAM, with remote devices that are controlled
in NCP mode.

SNA is, as its name implies, an architecture not a single
product. The products normally used to implement an SNA
network are Virtual Telecommunications Access Method
(VTAM) and Network Control Program (NCP).

SNA network characteristics

A component in an SNA network that implements the rules of
SNA is known as a node. Each node is represented by a
physical unit (PU).

The following three nodes are currently defined by SNA:

o      Host Node (HN)
o      Communications Controller Node (CUCN)
o      Peripheral Node (PN).

The host node will be the CPU and associated software, the
communications controller node will be the front-end
processor, and the peripheral node will be the terminal control
unit.

The HN is defined as the CPU running the operating system,
access methods such as VTAM, TP monitors such as CICS,
and the databases.

The CUCN is responsible for controlling the remote network
attached to it, when loaded with the NCP. It performs the
functions requested of it by the host, such as transmitting data.
Involved in the transmitting of the data are such functions as:

o      Polling and addressing of lines
o      Error recovery
o      Statistics gathering.

The PN provides access to the host for terminals and printers
connected to it. A PN usually consists of a microprocessor,
storage, and floppy and/or hard disk drives.

The lines between the CUCN and PN are controlled by the
SDLC protocol, which governs the format of the data, and
error recognition and recovery for line traffic.

SSCP (System Services Control Point)

SSCPs exist only in host processors and are the focal point for
network configuration management, problem determination,
and directory services. This is implemented in VTAM.

PU (Physical Unit)

Physical Units are components of the SNA network which
implement the SNA rules. In general PUs are control units such
as a 3174 or a 3745 running NCP. However some PUs such as
VTAM (VTAM is both an SSCP and a PU) are software
running in the host.

Depending upon its 'intelligence', each PU has a type. This is
illustrated in Figure 1.

LU (Logical Unit)

Logical units are generally end points in a network - the points
at which real work is done. Although attached to PUs, LUs can
only communicate directly with other LUs of the same type.

LUs can either be primary or secondary. A Primary LU (PLU)
can have multiple concurrent sessions, a Secondary LU (SLU)
only a single session. For instance, CICS is an LU and can
have multiple sessions with multiple terminals (also LUs),
therefore CICS is a PLU. Each of the terminals with which it
has a session will be an SLU. This is because those terminals
will be able to communicate only with that one CICS
application for the duration of the session. Figure 2 illustrates
LU types.

As with PU types, LU types depend upon the 'intelligence' of
the device or software product.

As LUs, applications can communicate with other LUs such as
terminals, thus following the rule that logical units can only be
in session with other logical units.

This leads to a problem: how does a printer LU type 3 and a
terminal LU type 2 (different LU types) both communicate
with the same CICS system, if the rule that LUs must be of the
same type for them to communicate is to be followed?

The answer is that products such as CICS must act like the LU
type of the resource communicating with it. So CICS etc must
all include functions to appear like any LU type likely to need
communications with it.

The VTAM interface referred to above comprises:

o      Resource configuration
o      Routeing of data
o      Resource control
o      Session control
o      Error recovery
o      Statistics gathering and generating
o      Operator interface.


VTAM UNDER VM

With the introduction of VM/SP Release 4 (and VM/SP HPO)
during 1986, native VM support for SNA was introduced (at
last). Products available include ACF/VTAM, RSCS, and
NetView. These products run in separate virtual machines, but
these virtual machines are themselves controlled by Group
Control System.

VM/GCS manages the SNA virtual machines, which are
allowed to share storage in order to communicate with one
another.

This still means that guest operating systems such as MVS will
need their own VTAM. There are several ways of connecting
them together, such as:

o      Multiple channel adapters to the 37xx
o      CTCA (virtual or real)
o      The VM dial facility.

Group Control System

The Group Control System (GCS) is:

o      An optional component of VM
o      A multitasking virtual machine supervisor
o      An interface between SNA program products, such as
      VTAM and RSCS, and the VM Control Program (CP).

GCS has one specific objective, ie to support a native
VM/SNA network.

A VM/SNA network relies on ACF/VTAM, VTAM SNA
Console Support (VSCS), and other network applications to
manage its collection of controllers, processors, and links
between terminals.

In turn, ACF/VTAM, VSCS, and the others rely on GCS to
provide services to them.

The exact flow of data through the system is given below.

VTAM, in the virtual machine, has the ability to control
terminals directly. VTAM can use real I/O, so the VM I/O
routines can be by-passed in the way they are in VSE/VTAM.
The VTAM machine handles terminals, CTCAs, and the FEPs.

VTAM will route the data to another node (eg a guest VTAM),
or to VSCS - a VTAM application running alongside VTAM in
the same virtual machine. VSCS then passes data via IUCV
(Inter User Communication Vehicle) to VM CCS (Console
Communication Services) which then passes the data to the
virtual machine the user is logged on to.

VM/ESA users can benefit from additional storage because
VTAM for VM/ESA supports 31 bit addressing.

NetView or Net/Master can run in a GCS controlled machine to
provide network management.

TSAF

Transparent Services Access Facility (TSAF) is an optional
component that provides LU6.2 services to virtual machine
applications.

It will let a user connect to a resource (such as an SQL
database) within a predefined group (a 'collection') of virtual
machines and resources, without needing to know the name of
the virtual machine that owns the resource or even the node
name of the target VM system.

Up to eight VM processors can combine to make a TSAF
collection. TSAF can dynamically configure a collection by
establishing communications with another TSAF virtual
machine.

The processors must be connected directly or indirectly by at
least one of the following:

o      Channel to Channel (CTC) links including 3088s and LANs
o      APPC/VM VTAM Support (AVS)
o      IBM Token Ring LAN adapter on a 9221 or 9370 processor
o      IEEE 802 Ethernet LAN adapter on a 9221 or 9370 processor.

For example, this could mean that an SQL application could
interrogate a database on another VM system if it couldn't
satisfy the request locally. This would be transparent to the
user.

Another example of how TSAF could be used is in the sharing
of CMS disk files across machines. In this case TSAF would
connect to the CMS Shared File System (SFS) server
machines.

APPC/VM VTAM Support

APPC/VM VTAM Support (AVS) is an optional component
that provides services allowing a program running in a TSAF
collection to communicate with another program running
anywhere on a VM or non-VM system in an SNA defined
network.

APPC/VM VTAM Support is an optional component of VM. It
is a VTAM application that runs under the control of GCS.

AVS, along with VTAM, provides the functions and facilities
necessary for APPC/VM programs within a TSAF collection
(or in a single VM processor) to communicate with
APPC/VTAM programs running anywhere within an SNA
defined network using SNA LU 6.2 protocols.

AVS and VTAM provide the SNA network with a view of the
TSAF collection. The collection looks like one or more SNA
Logical Units (LUs) to the network.

TCP/IP

VM provides support for Transmission Control
Protocol/Internet Protocol (TCP/IP), a protocol used in Unix
networks. In OSI terms, it implements the Network and
Transport layers (layers 3 and 4) of the OSI model. It is a
standard protocol used by the US Department of Defense
(although they are more likely to favour OSI in the future). For
physical data transmission, TCP/IP can use Token Ring or
Ethernet LANs.

An example of TCP/IP usage is an ES/9370 connected to a
number of Unix workstations via a LAN. Of course Unix
workstations and LANs can be IBM or non-IBM products.


NETWORK MANAGEMENT

Since networking can be complex, there are several network
control program products available to help the effective running
of a network. These are collectively known as Communications
Network Management software.

Products under this heading include NetView from IBM and
Net/Master from Systems Center.

Remote Spooling Communication Subsystem

Remote Spooling Communication Subsystem (RSCS) is a VM
networking program product. Its primary function is to move
data in the form of spool files between its system and remote
devices or other systems.

RSCS runs under the control of GCS.

The RSCS network consists of one or more systems, the I/O
devices, and the communication links that join them all
together.

The links can either be SNA or non-SNA controlled. The
points at which the links join or end are called nodes.

Nodes can be:

o      Local or remote
o      Adjacent or non-adjacent.

This allows data from one system to travel via another system
to end up at a target system or device.

Token Ring Network

The VM support for TRN comes in several forms. The TRN
can be connected via a 37xx, via a gateway PC, or via the 9221
TRN adapter. It is also possible to use TCP/IP for connection
to Ethernet, for example.

The software used to drive Token Ring is either NCP (if using
37xx), or, if using the 9221 TRN adapter, VTAM or TSAF.

PROP

PROP can be used in a network in conjunction with RSCS or
via NetView, using the PROP Message eXchange (PMX)
facility of NetView. In this way a VM system can be run
remotely from any NetView terminal.

Distributed Systems

VM systems can be used as distributed systems. A distributed
VM system could have software changes made by the remote
systems programmer simply logging on and making changes.
However, if there are many nodes to maintain, this may be
time-consuming.

MVS originally had DSX (Distributed Systems eXecutive) to
assist with this process. DSX offers assistance in change and
problem management, and distribution of software. DSX is
now replaced by NetView/DM (Distribution Manager) which
also runs under VM systems.

VM/DSNX (Distributed Systems Node eXecutive) resides in
the distributed nodes and receives objects from NetView/DM.

A change in VTAM is significant for distributed systems.
VTAM now supports switched subarea connections. In other
words, the central site personnel can dial up a remote 9221 to
carry out maintenance and tailoring, rather than having a
permanent connection. Another alternative for reducing line
costs is to allow multidrop connection of PU type 4 with other
physical units on non-switched lines.


VTAM UNDER VSE

Although adoption of VTAM in a native VSE environment was
several years behind MVS, most VSE installations are now
using VTAM.

CICS

Terminals are connected to VTAM, which allows a user to log
on to the CICS system of his or her choice. Terminals on the
same line can log-on to different systems, and, in multi-domain
or interconnected networks, users can log-on to any system.

POWER/RJE

SNA RJE or PNET can be run through VTAM. PNET can
submit jobs to other PNET systems, VM RSCS, or JES
(MVS).

OCCF

The Operator Communications Control Facility provides a
message code table which enables substitute messages (or no
message at all) to be sent to a remote site. Messages can also
be redirected to a NetView operator console if NetView is also
installed. OCCF also allows automatic message translation and
for the installation to predefine responses to messages.

One of the operational problems of a VM/VSE environment is
that it tends to be a multi-domain network. This immediately
causes a difficulty in network control. The best way of
managing this is to try to get all the terminals controlled by a
particular domain. Again, the ideal method would be through
VM/VTAM, but this is more expensive than choosing one of
the VSE systems. But VSE systems are more storage
constrained than VM ones. It would be feasible to have a VSE
system for network control, but the overheads and operational
complexity may be unacceptable.

Storage

A consideration for most VSE users is the amount of storage
that is to be used for the network software. A VTAM system
starts at about 2MB of storage and may need to increase for a
medium to large network. In addition to that, the other network
products, which become more of a requirement as the size of
the network increases, require storage. Using cross-domain
resources will also increase storage requirements. Native VSE
users can gain by moving to VSE/ESA and putting VTAM in a
private partition. VM users, who typically use 16MB systems,
will have to decide if they want to run a VSE/ESA system with
additional address spaces. If they use VM/ESA and have
PR/SM, then this may well be the best method using a V = R
guest.

Users of VM/VTAM will find themselves with much more
storage available, and therefore be able to support much larger
networks. It is VSE storage constraints that may make users
implement VM/VTAM for the majority of the network control,
with most terminals being owned by that domain, and the
network management products being run at the VM level.
However, the largest VSE CICS system will still have to be
managed carefully, to control the number of terminals defined
to it, and the number of cross-domain sessions allowed. The
use of CICS RDO, with automatic definition of TCT entries at
log-on time, enables the TCT requirements to be reduced. This
is particularly effective in VM or multi-domain environments.

VTAM interfaces

VTAM provides two special interfaces to assist with network
management; the Primary Operator Interface (POI) and
Communication Network Management (CNM) interface.

Communication network management software products use
these interfaces to communicate with VTAM. The POI allows
a software product to issue VTAM commands and receive
VTAM messages. The CNM interface allows the software
product to solicit and receive errors from the network.


NETVIEW

NetView is a blend of components that together provide a
comprehensive network management tool. NetView is
supported in MVS, VM, and VSE environments - although
some functions are not available in all environments or with
older releases.

NetView facilities

The basic features of NetView are:

o      Command facility
o      Session monitor
o      Hardware Monitor
o      Status Monitor
o      Panel manager
o      Logging
o      Network Asset Management
o      Automated operations
o      Cross-domain operation
o      LAN support.

There are a number of optional products which come under the
NetView umbrella. These include:

o      NetView/Access
o      NetView Distribution Manager
o      NetView Performance Monitor
o      NetView/File Transfer Program
o      NetView/PC.

NetView Version 1.1 provided all the facilities of NCCF,
NLDM, NPDA, TAF, and NMPF with some additions.
Version 1.2 added new functions and by then NetView looked
like an integrated product. Version 1.3 added REXX support.
Version 2 adds more function and the ability to use PS/2s for
graphical displays.

NetView operation

NetView operates as a VTAM application which can be
invoked from a 3270 terminal. The NetView operator simply
signs on and supplies a password using any VTAM terminal.
Each NetView user-id can be tailored for the scope of control
that the operator can have.

Access to NetView is controlled by its own user-id/password
system or optionally via RACF.

Command facility

At the most basic level, NetView provides for the input of all
VTAM operator commands from this terminal and reflects all
VTAM operator messages back to this terminal. Through its
multiple terminal support it allows control of the network to be
divided between multiple operators.

Each operator has an associated profile that controls most of his
or her NetView facilities and privileges.

Status Monitor

The Status Monitor displays network status and accepts
network operator commands. It provides an operator with
access to network data through a hierarchical set of interactive
display panels which can display the status of all resources in
the domain.

Cross-domain operation

NetView provides communication with other NetView systems
in other processors in the network. If there are many processors
in the network, network control can be centralized because
NetView allows a single operator to control the whole network.

Hardware Monitor

The Hardware Monitor collects and interprets information
about network component failures. It can also provide
threshold alert functions. Alerts identify serious problems so
that the operator can resolve them before they affect system
operation. The Hardware Monitor gives probable causes for a
failure with a suggested action list to solve the problem.

Session monitor

The session monitor collects information either from VTAM or
from another NetView. VTAM sends the session information
to NetView every time a session request is performed.

The session monitor can:

o      Collect and display data from the 3274/3174 Response
      Time Monitor facility.

o      Display session activity information from the same
      domain, cross-domain, or cross network environments.
      This data includes the particular route the session is using,
      addresses of session partners, and subarea information.

Browse facility

There is a browse function enabling network operators to
browse the network log for operator commands and messages.
Also datasets including those containing VTAM and NetView
definitions can be browsed.

Multiple sessions

The Terminal Access Facility (TAF) is a feature of NetView
which allows a single terminal to be simultaneously in session
with a number of applications without the need to log-off and
log-on repeatedly. This is a simplified form of multiple session
software.

Automated operations

The automation task enables NetView to respond to operating
system, subsystem, and network messages automatically.

In MVS, messages for MVS consoles can be processed
automatically by NetView. NetView command lists and
commands can be issued from MVS consoles.

NetView can perform in a similar way in VM and VSE systems
but not directly. In a VM system it interfaces with PROP
through a feature called PMX (PROP Message eXchange). In a
VSE system, NetView interfaces with OCCF in order to
control the VSE console. This function can be used with
remote systems to route certain messages through to a central
operator.

Network Asset Management

The Network Asset Management facility allows NetView to
obtain certain information from 3174 controllers and their
attached terminals. This information is known as Vital Product
Data.

DASD conservation

For remote VM and VSE sites, NetView can be installed with
many panels and some functions missing if the purpose of
NetView on that site is for an operator-less system. This saves
disk space for files that would never be used anyway.

Local Area Networks

LANs can pass error data through to NetView so that it can
alert the operator to LAN faults and identify where the fault is.
NetView can communicate with the IBM LAN Manager
program.

Additional NetView products

The products described below are add-ons to NetView, and are
separately chargeable.

NetView/Access

This is an optional product and provides a multiple application
interface allowing a user to sign on to a number of different
applications. A user profile can be stored in RACF, so that the
user only signs on once to NetView/Access and is then
automatically logged on to other applications that support auto
log-on. Movement between the different applications (TSO,
CICS, NetView etc) is via jump keys or menu selection.

SAMON

SNA Monitor is a product that can be run with or without
NetView. It provides a customized logo and assistance in
logging on to applications. The status of applications running
on the system can be displayed, and can even be set to be
automatically updated at all SAMON terminals at set intervals.

NetView/PC

NetView/PC provides a bridge between NetView and other
parts of a communications network, including voice networks,
non-IBM LANs, and OSI networks. It runs on a PS/2, and can
also support LAN Manager. NetView/PC is known to NetView
as a Service Point. Vendors or users can write software to send
CNM format messages to NetView.

NetView File Transfer Program

NetView FTP allows bulk transfer of data between MVS, VM,
and VSE systems.

NetView Performance Monitor

NPM is available for MVS and VM systems. The source of
NPM data is both from VTAM and NCP. NPM measures
response time and traffic volumes, and can provide graphic
displays if GDDM is available. It is also used for gathering
data for network planning.

Measurement can be reported on for different network
components such as LU, line, application, and response times.

NetView Distribution Manager

NetView DM is available for MVS and VM. It supersedes
DSX, and allows distribution of data objects (programs,
procedures, etc) in a controlled way. NetView DM can
download to VM systems (running VM/DSNX which receives
the data objects), VSE (running
VSE/DSNX), System/36s, AS/400s, PCs, etc.

NetView DM allows a 'central' 3174 to be used for remote
controller customization. A remote 3174 can have its
configuration carried out on the central one and the
configuration details are transmitted from the 'central' 3174 to
other 3174s using the host software.

NetView Network Definer

NetView Network Definer is a facility that assists in defining
VM SNA networks. It enables system personnel to use menus
to create VTAM, NetView, and RSCS definitions. It will also
ship definitions to nodes in the network and use its own
network naming standards (which can be overridden). This
product is aimed at 9221 VM networks.

Target System Control Facility

NetView also has an add on - TSCF (Target System Control
Facility) - which enables MVS and VM systems to be operated
remotely. This is done via a PC attached to the target host
(9021 or 9121). If your target system is VSE, then NetView
will allow this (with OCCF in the target system) in its standard
form. If your target is a 9221, then there are similar facilities
(standard except for remote power-on).

This article will be continued next month.

John Coe
Senior Consultant
RSM Consulting Ltd (UK)                                    ) John Coe 1992


