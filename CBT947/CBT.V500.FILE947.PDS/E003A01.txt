SQL/DS guest sharing with VM/VSE


INTRODUCTION

In late 1989 IBM announced the VM/VSE SQL/DS guest
sharing facility.  The guest sharing facility allows a VSE/SP (or
VSE/AF) system running as a guest under VM to share, with
CMS users, access to a VM SQL/DS database.

The mechanism employed for guest sharing is a special XPCC
(Cross Partition Communication Control) request in VSE that is
translated into an APPC (Advanced Program to Program
Communication) to VM.  Guest sharing is activated for batch
programs through a SET XPCC command when VSE is IPLd.
Guest sharing is activated for CICS programs through a
parameter in the CIRB transaction, which starts the on-line
resource manager for SQL/DS.

IBM provides an excellent guest sharing guide (GG24-3462-
00) that details all the possible combinations of VM and VSE
databases as well as providing tuning and installation planning
information.

My experience with guest sharing is as a consultant working
for a large scale, direct mail, fund raising organization.  My
client has an on-going commitment to implement relational
technology in an IBM mainframe environment.  Starting in the
spring of 1990 we planned for, installed, and implemented
guest sharing.  Our CPU is a 4381-P21.  We have about 32
on-line users (mix of CMS and CICS).  Our software is VM/IS
5.1, VSE/SP 4.1.2, and SQL/DS 2.2.0.  Our CMS users make
extensive use of ECF for query management and data transfer
from SQL/DS to PC, for use in software such as Lotus and
Quattro.

This article will cover three topics; our reasons for
implementing guest sharing, the process of installation and
implementation, and hints and tips on installation and use of
guest sharing.


WHY IMPLEMENT GUEST SHARING?

Until guest sharing, we had been maintaining our production
data in VSE/VSAM with a copy of the production data in
VM/SQL/DS.  We refreshed the SQL/DS tables nightly or at
weekends (depending on necessity of fresh data and the size of
the table) from the production VSAM files.  Periodic refresh
was an adequate solution since the first phase of our
implementation of relational technology was to supply decision
support capability.  Further, we had decided we must run
SQL/DS in VM because of greater vendor support, both from
IBM (eg ECF) and third parties.

We wanted to transfer 'ownership' of production data from
VSE/CICS/VSAM to VM/SQL/DS.  With VM/SQL/DS
owning the production data, we could eliminate the headaches
of maintaining redundant data, and save DASD resources.
Implementing guest sharing made this possible.

We also wanted to preserve our investment in application
software.  We have a highly customized data entry/file
maintenance system that we have developed over the last eight
years.  By implementing guest sharing and converting our
application programs to use SQL instead of VSAM, we have
taken another step towards full relational technology without
the expense of rewriting our entire system!

IBM made the decision to go to guest sharing easier by not
charging for both the VM and VSE databases.  The product
numbers are the same - are they trying to tell us something?

Advantages to us:

o      'Up-to-the-minute' data in SQL/DS.

o      Another step towards 'full' relational technology.

o      Simplified operation because maintenance of redundant
      data is no longer needed.

o      Reduction in DASD use because VSAM files are no longer needed.

o      The SQL/DS utility products available from both IBM and
      third parties are more robust for VM/SQL/DS.

Advantages to a VSE/SQL installation:

o      All of the above.

o      Reduction in virtual storage use with the elimination of one
      or more SQL/DS partitions.

o      Centralization of DBA functions under VM.

o      Better use of multiple CPUs.

Disadvantages.

o      VSE batch (including the SQL/DS preprocessor) can only
      access one VM database at a time.  This makes the test
      environment clumsy.

o      If SQL/DS is running in VSE and is accessed by CICS,
      batch can only access the VSE database.

o      Additional resource overhead of approximately 10% (IBM's number).

o      VSE cannot access a database running under VM/XA.

o      VSE running under VM/XA cannot access a VM/SP database.

Technical requirements

Level of VSE - 4.1.0 or higher.

Level of VM - VM/SP 5 (or 6 if you need to access a remote database).

Level of SQL/DS - 2.2.


Options for combinations of VSE and VM databases

Many combinations of VM and VSE databases are possible in a
guest sharing environment, with the following restrictions:

1      All batch programs running in a VSE guest have access to
      only one database (VM, through SET XPCC, or VSE, by default).

2      Each CICS partition can access only one SQL/DS database
      (VM or VSE) based on a parameter in CIRB (the
      transaction used to start the on-line resource manager).

3      If CICS is accessing a VSE database, batch can access
      only that database.

To change the database to which batch has access, VSE must
be IPLd with a different SET XPCC command.  To change the
database to which CICS has access, a CIRT transaction
(terminate on-line resource manager) must be issued, followed
by a CIRB with a different database name.

The approach we selected was to keep all databases centralized
under VM, thus providing us with all the advantages listed
above.  Moreover, since we never ran any VSE databases,
there was no migration concern.

I believe that any shop running multiple VSE guests with
several databases will find that by implementing guest sharing,
centralizing DBA in VM, and adding some of the powerful
after-market tools, they will realize significant reduction in
resource use and gains in productivity.


HOW INSTALLATION/IMPLEMENTATION WAS ACCOMPLISHED

Components of VM SQL/DS that were or weren't installed

Prior to implementing guest sharing, as part of our customary
system software upgrade policy, we had installed a fully
functional SQL/DS 2.2.0 in VM.

Components of VSE SQL/DS that were or weren't installed

We installed the database services utility, all the language
preprocessors (although we only use COBOL), ISQL, batch
resource manager, and the on-line resource manager.  We did
not install the database management modules and National
Language Support modules.  Installation and testing were done
in a test VSE/SP 4.1.2 guest so as not to interfere with
production.  The documentation we used was a combination of
the guest sharing guide, the SQL/DS Installation for VSE
manual, the Program Directory for use with SQL/DS and the
IBM SQL/DS Newsletter (Fall 1989).

How components were installed

First, we put this SET XPCC command in our IPL procedure:

DPD VOLID=DOSRES,CYL=423,TYPE=N,DSF=N
DLA NAME=AREA1,VOLID=DOSRES,CYL=60,NCYL=3,DSF=N
SET XPCC TARGET SYSARI TO APPCVM TARGET SQLDBA
SVA PSIZE=534K,SDL=250,GETVIS=64K

We used the TAILOR IPL PROCEDURE: VSE APPC/VM
SPECIFICATION panel of the VSE Interactive Interface to do this.

Although there was already an IUCV ALLOW statement in
SQLDBA's VM directory, we added the following IUCV
SQLDBA statement to VSE412's VM directory both to
document VSE412's guest sharing access and to ensure access
in the event of a change to SQLDBA:

USER VSE412  VSE412   8M 16M BG
IUCV SQLDBA
OPTION ECMODE BMX REALTIMER 370E MAXCONN 10
IPL CMS
CONSOLE 009 3215 T OPERATOR

Then we re-IPLd VSE.

Next, we performed the following steps as shown in the VSE
SQL/DS installation manual.

Prepare your VSE system

We installed our VSE/SP 4.1.2 system with configuration
number 3 (40M VAE, 12 partitions) so we already had
partition FA as the partition designated for SQL/DS with 4M of
virtual storage.  Further, the standard labels that come with the
3380 version of VSE/SP provide the SYS001 work space
required by this step.

Update the CICS tables for SQL/DS

We took the PCT and PPT entries straight out of the
installation manual.  Again, our SIT and TCT as installed had
all the required parameters (DBP=YES|xx, EXEC=YES,
EXITS=YES, TSP=YES|xx).

Define the library to be installed.

As noted in the installation manual this step is not needed for
VSE/SP users because the PRD2 library already exists.
However, we did define the PRD2.SQL220 sub-library (see
note under hints and tips).

Install (restore) the libraries via MSHP

The installation manual provides a sample job for doing this.
We used the INSTALL PROGRAM - STACKED V2
FORMAT panel of the VSE Interactive Interface to do this.
The Interactive Interface creates a large job-manager job stream
the critical step of which is this:

// ASSGN SYS006,1B0
// EXEC LIBR
CHANGE S=PRD2.SQL220 REUSE=IMMEDIATE
/*
// EXEC MSHP
INSTALL PRODUCT FROMT ID='SQL/DS.....2.2.0' PROD IN=PRD2.SQL220
/*
// EXEC LIBR
CHANGE S=PRD2.SQL220 REUSE=AUTOMATIC
/*

After installing SQL/DS in PRD2.SQL220, we added
PRD2.SQL220 to our ICCF/CICS start-up LIBDEF so we
could use LIBRx commands in ICCF to view/retrieve its
contents.  We then re-cycled ICCF/CICS.

Catalogue the base procedures

We catalogued the base procedures for VSE/SP using the
ARIS223D.Z and ARIS22AD.Z jobs provided in
PRD2.SQL220.

Perform SQL/DS system installation link edits

We ran the link edit using the ARIS22BD.Z job provided in
PRD2.SQL220.
Since we were not installing a VSE database, steps 7 through
10 (Optionally define the VSAM master catalog, define VSAM
datasets for the database, generate the initial database, install
optional database components) were not done.

Optionally GRANT SCHEDULE authority

We performed the GRANT SCHEDULE using the
ARIS22FD.Z job provided in PRD2.SQL220.  We changed the
CICS appl-id and the SQLDBA password as needed.

Start SQL/DS in multi-user mode

It was unnecessary to start SQL/DS in multi-user mode because
SQL/DS was already running in VM.  We then ran the CIRB
and ISQL transactions in CICS to test the installation, and
everything worked fine.  We also wrote a quick batch COBOL
program to query one of our tables, preprocessed it, compiled
it, and ran it.  The batch COBOL also worked fine.  Since our
tests ran well, we didn't bother with steps 13 and 14 of the
installation (running ISQL against some system tables and
running the sample COBOL program ARISCBLD).

After the installation we began converting our programs from
VSAM to use SQL.  This is proving to be a straightforward
project although this is our first experience with the
combination of SQL and COBOL.


HINTS AND TIPS

The initial installation went well and was accomplished,
including preparatory work, in one day.  However, because the
VSE/SQL/DS installation manual is meant for a VSE/AF shop
(as opposed to VSE/SP), some interpretation was required.  In
fact, the installation guide would be very confusing to a novice
VSE/SP system programmer.  For example, it provides JCL to
define a VSAM master catalog in step 7, yet discusses defining
a VSAM managed library in step 3!  Further, the installation
manual offers only cursory advice as to which steps to skip or
modify if one is installing under VSE/SP.  I feel very strongly
that IBM should provide specific VSE/SP instructions for
installing both SQL/DS and guest sharing.

One problem we encountered early on was choosing the sub-
library to install SQL/DS in.  The VSE/SP manuals all
reference PRD2.DBASE as the database sub-library.
However, the VSE/SQL/DS installation manual exclusively
references PRD2.SQL220 as the database sub-library.  As
mentioned before, we elected to define the PRD2.SQL220 sub-
library.  Also, using the PRD2.SQL220 sub-library instead of
PRD2.DBASE allows one the flexibility to up-level SQL/DS
using a different sub-library (eg PRD2.SQL310).

Another problem we encountered almost right away was a
shortage of real memory allocated to our partitions.  The
default configuration (see above) we used when installing
VSE/SP called for 0K of real memory in our batch partitions.
The guest sharing guide briefly mentions real memory,
recommending 32K per path into SQL/DS per partition
accessing SQL/DS.  We found that a minimum of 64K per
batch partition and an additional 96K in our CICS partitions
was needed for reliable guest sharing access.  So, we
configured our partitions as follows:

ALLOC S,F1=768K,F3=3072K,F5=256K
ALLOC 1,BG=1536K,F7=1024K,F8=4608K,F4=1024K
ALLOC 2,F2=7878K,F6=348K
ALLOC 3,F9=2048K,FA=5120K,FB=2048K
ALLOC R,BG=64K,F1=64K,F2=256K,F3=424K
ALLOC R,F4=64K,F5=128K,F6=64K,F7=64K
ALLOC R,F8=256K,F9=64K,FA=128K,FB=64K

Still another problem that plagued us for several days was the
inability to establish more than 4 paths into the database
machine from VSE.  The solution to this problem was to
increase the MAXCONN parameter of the OPTION statement
in VSE412's VM directory to 10 from the default of 4 (see
directory extract above).  The potential need to increase this
parameter is not mentioned anywhere in the guest sharing
guide, installation manual, or the newsletters.

Yet another problem we soon encountered was preprocessing
and compiling a COBOL program that uses the EXEC SQL
interface.  The SQL/DS Application Programming for VSE
manual supplies simple models for doing this for both batch
and CICS.  However, the models use sequential disk extents
for the output of the SQL/DS preprocessor (input to the
COBOL compiler or CICS preprocessor).  We don't like
sequential disk extents for utility functions like compiles
because it means we must have a different set of extents or
even a different JCL for each partition.  Further, the CICS
model is unclear as to whether the INCLUDE statements for
the SQL/DS modules ARIRRTED and ARIPADR belong after
the execute of FCOBOL or before the execute of LNKEDT.

After some experimentation we came up with the following
batch SQL/COBOL compile skeleton.  This skeleton uses the
POWER DISP=I punch capability to punch the output of the
SQL/DS preprocessor behind some compile JCL thus allowing
us to run SQL/COBOL compiles in any and all partitions.  By
placing this compile skeleton in member C$$COBAT of the
ICCF library in which our batch SQL/COBOL is kept, we can
use option 8 (compile) of the Interactive Interface PRIMARY
LIBRARY panel for our batch SQL/COBOL compiles.

* $$ JOB JNM=&JOBNAME,DISP=D,CLASS=A,NTFY=YES
* $$ LST DISP=D,CLASS=Q,PRI=3
* $$ PUN DISP=I,PRI=9,CLASS=A
// JOB &JOBNAME SQL PREP PROGRAM &PROGNAME
// ASSGN SYS005,SYSRDR
// EXEC IESINSRT
$ $$ LST DISP=D,CLASS=Q,PRI=3
// JOB &JOBNAME COMPILE PROGRAM &PROGNAME
// SETPARM CATALOG=&CATALOG
// IF CATALOG = 1 THEN
// GOTO CAT
// OPTION ERRS,SXREF,SYM,LIST,NODECK
// GOTO ENDCAT
/.  CAT
// LIBDEF PHASE,CATALOG=FRISPRD.LOADLIB
// OPTION ERRS,SXREF,SYM,CATAL,NODECK
   PHASE &PROGNAME,*
   INCLUDE ARIPRDID
   INCLUDE ARIPADR
/.  ENDCAT
// EXEC FCOBOL,SIZE=256K
 CBL LIB,FLOW,STATE,VERB,LANGLVL(1),APOST,NOTRUNC,NOADV,BUF=2048
* $$ END
// ON $CANCEL OR $ABEND GOTO ENDJ2
// EXEC
ARIPRPC,SIZE=AUTO,PARM='USERID=VSEBATCH/VSEBATCH,KEEP,ISOL(CS),*
               PREPNAME=&PROGNAME,BLOCK'
* $$ SLI ICCF=(&PROGNAME,&PASSWORD),LIB=&LIBNO
/*
/.  ENDJ2
// EXEC IESINSRT
/*
// IF CATALOG NE 1 OR $MRC GT 4 THEN
// GOTO NOLNK
// EXEC LNKEDT,SIZE=256K
/.  NOLNK
#&
$ $$ EOJ
* $$ END
/&
* $$ EOJ

After some experimentation we came up with the following
CICS SQL/COBOL compile skeleton.  As with the batch
compile, this skeleton uses the POWER DISP=I punch
capability to punch the output of the SQL/DS preprocessor
behind some CICS preprocessor which in turn punches its
output behind some compile JCL.  This skeleton also uses a
quick COBOL program (KDKINSRT) we wrote to read JCL
from the job stream and punch it out.  We wrote KDKINSRT
because we couldn't get IESINSRT (the similar function
supplied with VSE) to do what we wanted in this case.  By
placing this compile skeleton in member C$$COONL of the
ICCF library in which our CICS SQL/COBOL is kept, we can
use option 8 (compile) of the Interactive Interface PRIMARY
LIBRARY panel for our CICS SQL/COBOL compiles.

* $$ JOB JNM=&JOBNAME,DISP=D,CLASS=A,NTFY=YES
* $$ LST DISP=D,CLASS=Q,PRI=3
* $$ PUN DISP=I,PRI=9,CLASS=A
// JOB &JOBNAME SQL PREP PROGRAM &PROGNAME
// ASSGN SYS006,FED
// EXEC KDKINSRT
$ $$ LST DISP=D,CLASS=Q,PRI=3
$ $$ PUN DISP=I,PRI=9,CLASS=A
// JOB &JOBNAME TRANSLATE PROGRAM &PROGNAME
// ASSGN SYS005,SYSRDR
// EXEC IESINSRT
@ $$ LST DISP=D,CLASS=Q,PRI=3
// JOB &JOBNAME COMPILE PROGRAM &PROGNAME
// SETPARM CATALOG=1
// IF CATALOG = 1 THEN
// GOTO CAT
// OPTION ERRS,SXREF,SYM,LIST,NODECK
// GOTO ENDCAT
/.  CAT
// LIBDEF PHASE,CATALOG=FRISPRD.LOADLIB
// OPTION ERRS,SXREF,SYM,CATAL,NODECK
   PHASE &PROGNAME,*
   INCLUDE DFHECI
/.  ENDCAT
// EXEC FCOBOL,SIZE=256K
 CBL LIB,LANGLVL(1),APOST,NOADV,BUF=2048
$ $$ END
// ON $CANCEL OR $ABEND GOTO ENDJ2
// OPTION NOLIST,NODUMP,DECK
// EXEC DFHECP1$,SIZE=512K
 CBL XOPTS(CICS DEBUG LANGLVL(1))
@ $$ END
// ON $CANCEL OR $ABEND GOTO ENDJ2
// EXEC
ARIPRPC,SIZE=AUTO,PARM='USERID=CICSUSER/CICSUSER,KEEP,ISOL(CS),*
              PREPNAME=&PROGNAME,BLOCK'
* $$ SLI ICCF=(&PROGNAME),LIB=0029
/*
/.  ENDJ2
// ASSGN SYS006,FED
// EXEC KDKINSRT
#*
/.  ENDJ2
// EXEC IESINSRT
+*
// IF CATALOG NE 1 OR $MRC GT 4 THEN
// GOTO NOLNK
  INCLUDE ARIRRTED
  INCLUDE ARIPADR
// EXEC LNKEDT,SIZE=256K
/.  NOLNK
+&
@ $$ EOJ
$ $$ END
#&
$ $$ EOJ
@ $$ END
/&
* $$ EOJ


KDKINSRT

CBL APOST,LIB,NOSYM,SXREF,CLIST
      IDENTIFICATION DIVISION.
      PROGRAM-ID.    KDKINSRT.
      REMARKS.  CARD-TO-CARD CONVERT @ TO $ AND + TO # IN COL 1.
                          CAN'T FIND DOCS ON IBM'S IESINSRT TO SEE IF IT
                          CAN DO THIS.
                 ENVIRONMENT DIVISION.
                 INPUT-OUTPUT SECTION.
                 FILE-CONTROL.
                     SELECT CARD-OUT  ASSIGN TO SYS006-UR-2540P-S.
                 DATA DIVISION.
                 FILE SECTION.
                 FD  CARD-OUT
                        RECORDING MODE IS F
                        LABEL RECORDS ARE OMITTED
                        RECORD CONTAINS 080 CHARACTERS
                        DATA RECORD IS OP-REC.
                 01  OP-REC                      PIC X(080).
                 WORKING-STORAGE SECTION.
                 01  WORK-AREA.
                     02  FILLER   PIC X(16)  VALUE 'WORKING STORAGE '.
                     02  W2-IP-CNT          PIC S9(7) COMP-3 VALUE +0.
                     02  W2-OP-CNT          PIC S9(7) COMP-3 VALUE +0.
                 01  IP-REC.
                     05  IP-END.
                       10  IP-CONVERT5
                         15  IP-CONVERT3         PIC XXX.
                         15  FILLER              PIC XX.
                       10  FILLER                PIC XXX.
                     05  FILLER                  PIC X(72).
                 PROCEDURE DIVISION.
                 HSK.  OPEN OUTPUT CARD-OUT.
                 P1-READ.
                     ACCEPT IP-REC.
                     ADD 1 TO W2-IP-CNT.
                     IF IP-END EQUAL '@ $$ END'
                        GO TO P99-EOF.
                     IF IP-CONVERT5 EQUAL '@ $$ '
                        MOVE '$ $$ ' TO IP-CONVERT5
                     ELSE
                     IF IP-CONVERT5 EQUAL '$ $$ '
                        MOVE '* $$ ' TO IP-CONVERT5
                     ELSE
                     IF IP-CONVERT3 EQUAL '+* '
                        MOVE '#* ' TO IP-CONVERT3
                     ELSE
                     IF IP-CONVERT3 EQUAL '#* '
                        MOVE '/* ' TO IP-CONVERT3
                     ELSE
                     IF IP-CONVERT3 EQUAL '+& '
                        MOVE '#& ' TO IP-CONVERT3
                     ELSE
                     IF IP-CONVERT3 EQUAL '#& '
                        MOVE '/& ' TO IP-CONVERT3.
                     MOVE IP-REC TO OP-REC.
                     WRITE OP-REC.
                     ADD 1 TO W2-OP-CNT.
                     GO TO P1-READ.
                 P99-EOF.
                     CLOSE CARD-OUT.
                     STOP RUN.

In conclusion, we have found guest sharing to be a good match
for our requirements, we have had no insurmountable problems
with its installation and use and it moves us forward in our
effort to implement a full relational environment.

Kevin Kitowski
Brickhouse Associates (USA)                  cBrickhouse Associates 1991

