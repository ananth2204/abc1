Relieving spool congestion

There are a number of ways of relieving spool congestion.  If there are
a lot of TSO sessions, commands such as $PT1-9999, entered regularly and
automatically, will delete session output.  This will not affect active
sessions but if users allocate SYSOUT datasets to their sessions these
will be deleted also.  The spool off-load and restore facility can be
used to dump and subsequently reload jobs from the spool.  The MVS
external writer can be used to select the output of a given class and
write it to disk or tape.  This can be returned to the spool by simply
copying the file created in this way to SYSOUT.

One of the main problems with tuning JES is that it produces no
statistics on its own performance.  The type 26 SMF record, which is
written when all the datasets from a job are purged from the spool,
contains some information on spool usage such as the number of spool
bytes allocated to a job, lines of output it contained etc.  Collection
of the type 26 records is achieved by specifying TYPE26=YES (the
default) on the JOBCLASS(c), TSUCLASS, and STCCLASS definition
statements and amending the SUBSYS(STC(...)) definition in the SMFPRMxx
member of PARMLIB.  The STC definition must be amended because, even
though the SMF records concern batch jobs or TSO sessions, the records
are stored on the spool and written by JES when the last print dataset
is purged.

The following SAS code illustrates some simple processing of type 26
records.  It produces a table of spool usage in Kbytes by duration for
jobs, TSO sessions, and started tasks.

DATA T26 (KEEP=SYSTEM PURGTIME ENDTIME NAME TYPE SPLDURTM
               SPOOLBYT SPLBYTGP SPLDURGP);
LENGTH DEFAULT=4 PURGTIME ENDTIME 8;
FORMAT PURGTIME
       ENDTIME DATETIME19.2
       SPLDURTM TIME8.;
INFILE SMF STOPOVER LENGTH=LENGTH COL=COL RECFM=VBS LRECL=32756;
INPUT @2 ID PIB1. @;
IF ID=26;
/* TYPE 26 RECORDS ARE 352 BYTES IN LENGTH (LESS RDW) */
INPUT @3 PURGTIME SMFSTAMP8. @11 SYSTEM $4.
      @15 NAME $8. @57 TYPE $1. @193 ENDTIME SMFSTAMP8.
      @225 SPOOLINE PIB4. @333 SPOOLBYT PIB4.;
/* SPLIT PURGE TIMES AND END EXECUTION TIMES INTO HOURS, MINUTES */
/* AND SECONDS AND CALCULATE DIFFERENCE = DURATION ON SPOOL      */
HRD=HOUR(PURGTIME)-HOUR(ENDTIME)
    + (DATEPART(PURGTIME)-DATEPART(ENDTIME))*24;
MIND=MINUTE(PURGTIME)-MINUTE(ENDTIME);
SECD=SECOND(PURGTIME)-SECOND(ENDTIME);
IF SECD<0 THEN DO;
  SECD+60;
  MIND=MIND-1;
END;
IF MIND<0 THEN DO;
  MIND+60;
  HRD=HRD-1;
END;
SPLDURTM=HMS(HRD,MIND,SECD);
SPOOLBYT=ROUND(SPOOLBYT/1024);
/* SPLIT DURATION INTO GROUPS                                    */
IF HRD = 0 THEN SPLDURGP=1;
ELSE IF HRD = 1 THEN SPLDURGP=2;
ELSE IF HRD = 2 THEN SPLDURGP=3;
ELSE IF HRD = 3 THEN SPLDURGP=4;
ELSE IF HRD = 4 THEN SPLDURGP=5;
ELSE IF HRD = 5 THEN SPLDURGP=6;
ELSE IF 6 <= HRD < 8 THEN SPLDURGP=7;
ELSE IF 8 <= HRD < 10 THEN SPLDURGP=8;
ELSE IF 10 <= HRD < 12 THEN SPLDURGP=9;
ELSE IF 12 <= HRD < 14 THEN SPLDURGP=10;
ELSE IF 14 <= HRD < 16 THEN SPLDURGP=11;
ELSE IF 16 <= HRD < 20 THEN SPLDURGP=12;
ELSE IF 20 <= HRD < 24 THEN SPLDURGP=13;
ELSE IF 24 <= HRD < 28 THEN SPLDURGP=14;
ELSE IF 28 <= HRD < 32 THEN SPLDURGP=15;
ELSE IF 32 <= HRD < 36 THEN SPLDURGP=16;
ELSE SPLDURGP=17;
/* SPLIT SPOOL KBYTES USED INTO GROUPS                           */
IF SPOOLBYT < 10 THEN SPLBYTGP=1;
ELSE IF 10 <= SPOOLBYT < 20 THEN SPLBYTGP=2;
ELSE IF 20 <= SPOOLBYT < 50 THEN SPLBYTGP=3;
ELSE IF 50 <= SPOOLBYT < 100 THEN SPLBYTGP=4;
ELSE IF 100 <= SPOOLBYT < 250 THEN SPLBYTGP=5;
ELSE IF 250 <= SPOOLBYT < 500 THEN SPLBYTGP=6;
ELSE IF 500 <= SPOOLBYT < 1000 THEN SPLBYTGP=7;
ELSE IF 1000 <= SPOOLBYT < 2000 THEN SPLBYTGP=8;
ELSE IF 2000 <= SPOOLBYT < 4000 THEN SPLBYTGP=9;
ELSE IF 4000 <= SPOOLBYT < 8000 THEN SPLBYTGP=10;
ELSE IF 8000 <= SPOOLBYT < 12000 THEN SPLBYTGP=11;
ELSE SPLBYTGP=12;
OUTPUT T26;
RETURN;
/* SORT BY TYPE BECAUSE TABULATES PAGES BY TYPE                  */
PROC SORT; BY TYPE;
/* DURATION AND SPOOL GROUP AND TYPE FORMATS FOR PRINTING        */
PROC FORMAT;
 VALUE DURGRP 1='>1 HOUR'
              2='>1 HOUR & <2 HOUR'
              3='>2 HOUR & <3 HOUR'
              4='>3 HOUR & <4 HOUR'
              5='>4 HOUR & <5 HOUR'
              6='>5 HOUR & <6 HOUR'
              7='>6 HOUR & <8 HOUR'
              8='>8 HOUR & <10 HOUR'
              9='>10 HOUR & <12 HOUR'
              10='>12 HOUR & <14 HOUR'
              11='>14 HOUR & <16 HOUR'
              12='>16 HOUR & <20 HOUR'
              13='>20 HOUR & <24 HOUR'
              14='>24 HOUR & <28 HOUR'
              15='>28 HOUR & <32 HOUR'
              16='>32 HOUR & <36 HOUR'
              17='>36 HOUR';
 VALUE BYTGRP 1='<10K'
              2='>10K & <20K'
              3='>20K & <50K'
              4='>50K & <100K'
              5='>100K & <250K'
              6='>250K & <500K'
              7='>500K & <1,000K'
              8='>1,000K & <2,000K'
              9='>2,000K & <4,000K'
              10='>4,000K & <8,000K'
              11='>8,000K & <12,000K'
              12='>12,000K';
 VALUE $TYPE T='TSO SESSIONS'
             J='BATCH JOBS'
             S='STARTED TASKS';
/* PRODUCE TABLE OF SPOOL DURATION BY SPOOL USE AND GIVE         */
/* SEPARATE TABLE FOR BATCH JOBS, TSO SESSIONS AND STARTED       */
/* TASKS.                                                        */
PROC TABULATE;
BY TYPE;
FORMAT SPLDURGP DURGRP.
       SPLBYTGP BYTGRP.
       TYPE $TYPE.;
CLASS TYPE SPLDURGP SPLBYTGP;
/* ANALYSIS VARIABLE MUST BE NUMERIC - PURGTIME USED ONLY FOR    */
/* CONVENIENCE. IT IS NOT PRINTED - JUST TOTALS ARE CALCULATED   */
VAR PURGTIME;
TABLE TYPE, SPLDURGP, SPLBYTGP*(PURGTIME*N*F=10.0);
KEYLABEL N=TOTAL;
LABEL PURGTIME='TASKS'
      SPLDURGP='DURATION ON SPOOL'
      SPLBYTGP='SPOOL BYTES';
TITLE 'SPOOL USE BY DURATION';

The article in the December 1987 issue of MVS Update on JES2 spool
allocation described the use of the TGSIZE parameter but did not go into
the other parameters which affect spool allocation and utilisation, and
the performance and resource utilisation implications of making changes
to these parameters.

The version of JES to which this article applies is 1.3.6 (which is
functionally equivalent to JES 2.1.5).

In general, the spool can be allocated over multiple volumes but each
spool file must be in a single extent.  A common configuration is to
have the spool on a single 3380-type disk with a single-track VTOC on
track 1 of cylinder 0 (after the volume label) with the remaining space
(13273 tracks in the case of a single density disk) allocated to the
spool.

The parameters affecting spool allocation and utilisation are:

*    BUFSIZE - This is the size of a spool buffer in bytes.  The spool
is formatted into buffers of this size.  I/O operations are also
performed on buffers.  The default size is 3992, which ensures that a
buffer and its associated IOB will fit in a 4K page.  Using this default
value will allow 10 buffers per 3380 track, which makes 84% use of the
disk space.  However, it makes good use of scarcer real storage.  It is
the recommended value and it can only be modified at cold start.

*    TGSIZE - This is the number of buffers in a track group.  Each
batch job, TSO session, or started task is allocated spool space in
track groups.  The default TGSIZE value is 30 buffers, which, with the
default BUFSIZE, is three 3380 tracks.  It can be modified at warm
start.

*    TRKCELL - This is the number of buffers in a track cell.  Track
celling is both a spool organisational parameter and a performance
option for locally attached printers.  It is meant specifically for
high-speed printers such as 3800s.  As a spool parameter it applies to
print classes which have TRKCELL=YES (the default) specified on their
OUTCLASS(c) definition statement.  When DSPLTCEL is specified on the
PRTnnn definition statement (the default is DSPLSNGL), print datasets
for classes that are defined with TRKCELL=YES are de- spooled to the
printer in track cells rather than in single buffers.  It can operate
for all locally-attached printers even though they may not be able to
take advantage of it.  Data is still de-spooled in cells and I/O to the
spool is consequently reduced.  However, JES fixes printer buffers in
storage.  On the spool, tracks are divided into cells.  The size of a
cell cannot exceed the number of buffers on a track - if it does, the
cell consists of the whole track.

After the subdivision of a track, any buffers that are left over are
considered as a new cell if their number is greater than or equal to
half the TRKCELL value.  If the number left over is less than this,
those buffers are not considered as a cell and are only available for
print datasets in the job which occupies the track group within which
are the cell and the print classes that do not have TRKCELL=YES defined.
The default TRKCELL value is 3, which means that with the default
BUFSIZE a track consists of three cells and one buffer left over.  The
left-over buffer will not be used for print datasets with TRKCELL=YES
since it is less than half the cell size.  So, if the default values are
used for BUFSIZE, TRKCELL, and OUTCLASS(c) TRKCELL, 10% of the spool
will be unable to be used.  TRKCELL can only be changed at cold start.

These spool allocation parameters must be considered with reference to
the type of work in an installation.  If there are a lot of jobs that
produce a small amount of output (such as TSO sessions that do not
produce print datasets or allocate many files) the spool utilisation
will be artificially high with a lot of spool space being allocated but
unused.  The default TGSIZE of 30 buffers of 3992 bytes may be too high.
When my installation was replacing disks recently,

I conducted a number of tests on JES spool performance and utilisation
for a variety of TGSIZE and TRKCELL values.  For each combination, a
series of 400 jobs that produced 3.12 million lines of output was run.
The job stream consisted of single and multiple step jobs that produced
single and multiple print files of varying sizes.  The length of the
print records was both fixed (in a variety of lengths) and variable.
Most of the print lines were quite full - at least 80 bytes in length -
so the percentage spool utilisation achieved for the 3.12 million lines
of output will not be an accurate reflection of the actual spool
capacity but the relative percentage spool utilisation will still be
valid.  For the test, the spool was on a 3380D-type disk and was
allocated the full disk, as described above.  The checkpoint dataset was
also on a 3380D disk.  During the test no printers or other devices were
active.  Only one other task, a monitor, was active.  Since during the
test a large number of batch jobs were writing large amounts of output
to the spool, spool device busy and service time figures were very high.
Again, the relative values are important.  Figure 1 outlines the results
of the tests.  The measures given are:

% SPOOL UTILISATION - Percentage spool utilisation returned by the
$DSPOOL command.

SPOOL SIOs - The number of SIOs issued to the spool disk.

SPOOL % DEVICE BUSY - Percentage of the time during the test the spool
disk was busy.

SPOOL TOTAL SERVICE TIME - Average time to service an I/O request plus
average time spent in queue.

SPOOL QUEUE SERVICE TIME - Average amount of time I/O requests spent
queued for spool disk.

SPOOL SERVICE TIME - Average I/O request service time.

AVG QUEUE DEPTH - Average number of I/O requests spent queued for spool
disk.

CHKPT SIOs - The number of SIOs issued to the disk containing the
checkpoint dataset.

CHKPT % DEVICE BUSY - Percentage time during the test the checkpoint
dataset disk was busy.

CHKPT TOTAL SERVICE TIME - Average time to service an I/O request plus
average time spent in queue.

JES AVG PVT AREA FRAMES - Average number of allocated page frames for
the JES address space.

JES MAX PVT AREA FRAMES - Maximum number of allocated page frames for
the JES address space.

JES TCB SERVICE/1000 - JES performance group TCB service units/1000.
JES SRB SERVICE/1000 - JES performance group SRB service units/1000.
JES I/O SERVICE/1000 - JES performance group I/O service units/1000.
JES MEM SERVICE/1000 - JES performance group storage service units/1000.
JES TOTAL SERVICE/1000 - JES performance group total service units.

Alan McSweeney
Systems Programmer
Central Computing Service (Eire)   € Xephon 1988



























