Network performance monitors - a survey

The following article is taken from the findings of Xephon's
IBEX research programme, a series of regular surveys covering
all aspects of the computer industry directly related to IBM and
IBM-compatible mainframe installations throughout the world.

We asked those taking part in the survey if they "have a
VTAM or network performance monitor (eg NetSpy, NetView
PM, Omegamon for VTAM etc)". 30% of the 910 sites that
responded to this part of the questionnaire have one or more
network monitors - a significant increase on the 23% of sites
that had network performance monitors in a similar survey we
conducted last year. Figure 1 shows the distribution of
packages reported by five or more members at sites classified
by principal operating system. The most widespread network
monitor in the sample is IBM's NetView Performance Monitor,
which was previously known as Network Performance
Monitor, or NPM, and is a replacement for IBM's NPA
(Network Performance Analyser). NetView PM is installed at
15% of sites and accounts for 47% of licences. Legent's
NetSpy is the second most widely installed package in the
survey, being reported by 10% of sites in the sample, and
accounts for 32% of licences. Other packages installed at five
or more sites are: Candle's Omegamon II for VTAM, which  is
installed at 3% of sites and accounts for 9% of licences;
BlueLine Software's Vital Signs for VTAM, which is installed
at 2% of sites and accounts for 6% of licences; and Landmark
Systems' The Monitor for VTAM, which is installed at a little
under 1% of sites and accounts for 3% of licences.

Figure 2 shows the percentage of sites (broken down by
principal operating system, industry, and region) that have
network performance monitors installed. Network monitors are
installed at almost 50% of MVS/ESA sites (regardless of which
version they have), at around 40% of MVS/XA sites, but at
only 8% of all other sites. The breakdown by industry sector
shows these packages are less popular at manufacturing, health
care, and educational sites than at other sites, while the
breakdown by region shows little significant difference in the
uptake of this technology at sites in different locations.

In addition to the 272 sites in our sample with network
performance monitors, 106 sites (12% of those in the survey)
are thinking of acquiring such packages in the coming year,
while 27 sites (10% of those with packages) are thinking of
replacing their current package. Figure 3 shows how these
planned installations and replacements would affect the user
bases of packages in our sample. 73 of the 133 sites acquiring
packages in the next twelve months have as yet not chosen a
package - for this reason, Figure 3 shows the effect of
assigning these 'undecided' licences to packages pro-rata to the
prospective customers that have already chosen each.

NetSpy looks set to gain most in the coming year - 19
members are thinking of installing it and only three are
thinking of replacing it, which would result in an extra 16
licences. Significantly, 11 members are thinking of acquiring
NetSpy as a replacement for NetView PM. By assigning
NetSpy its share of the 'undecided' licences (as explained
above), we estimate that NetSpy may attract another 23
licences (among our sample sites), resulting in a net gain of 39
licences in the coming year.

Omegamon II for VTAM has been chosen by seven members
acquiring network monitors in the coming year (including three
who are replacing NetView PM), while one member is thinking
of replacing it. This would result in a net gain of six licences at
sites in the sample. Omegamon's share of the 'undecided'
licences would yield another eight licences, resulting in an
overall net gain of fourteen.

Four members are considering installing Vital Signs for VTAM
(two of them as replacements for NetView PM), while one
member is considering replacing it - a net gain of three
licences. Vital Signs' share of the 'undecided' licences would
yield five extra licences, giving it an overall net gain of eight.

Three members are considering installing The Monitor for
VTAM (one of them as a replacement for NetView PM), and
one member is considering replacing it - a net gain of two
licences. The Monitor's share of the 'undecided' licences is
five, resulting in an overall net gain of six.

No fewer than 23 sites, none of which currently has a network
performance monitor, are considering installing NetView PM.
However, 21 members who currently use NetView PM are
considering replacing it - a net gain of only two licences.
Nevertheless, we estimate that NetView may attract another 28
licences from members currently undecided about which
package to acquire, which would yield an overall net gain of
around thirty licences.


Market shares

We asked members in which year they installed their network
performance monitors. Figure 4 shows the installed bases of
the leading packages at sample sites each year since 1986 based
on these responses, while Figure 5 shows the corresponding
market shares. Both charts exclude packages for which we
were not given an acquisition date, and hence somewhat
underestimate the installed bases of packages. Readers should
also note that the number of 'Future' licences in Figures 4 and
5 is calculated by adding all sites planning to acquire a package
in the next year and subtracting the number planning to replace
it. Prospective customers who have yet to choose a package are
shown as 'unspecified', and are not allocated to other packages
as in Figure 3.

The market for network performance monitors appears to have
been growing exponentially up to 1990, at which time it
slowed down considerably, no doubt as a result of the slow-
down in the mainframe market as a whole. Nevertheless, the
market for these packages appears to have grown by around
16% last year, and there is considerable interest in acquiring
these packages in the coming year.

NetView PM and NetSpy both benefited from the strength of
the market for network monitors prior to 1990, by which time
both packages had established over half their current user-
bases. Since 1989, NetSpy has been steadily gaining market
share at the expense of NetView PM, and looks set to overtake
the latter in the next few years. Omegamon has chalked up
steady sales in the last few years - its market share has grown
for the last two years, and looks set to grow again in the
coming year.


Competitive evaluations

We asked members if they had conducted an evaluation
involving two or more packages before acquiring their current
one. Figure 6 shows the percentage of currently installed
packages that were involved in such evaluations (the chart only
explicitly lists packages for which ten or more members
reported whether an evaluation had taken place).

Almost three-quarters of NetSpy's users conducted an
evaluation before selecting it, while, at the other extreme, only
17% of NetView PM's licences resulted from evaluations.
NetSpy is also very successful in evaluations, winning eleven
out of every twelve - see Figure 7. NetView PM, by contrast,
has a somewhat poor record in evaluations. Figure 8 shows the
results of all 124 evaluations reported by members. Overall,
the percentage of network performance monitors that were
selected after evaluations has increased from 30% in our
October 1990 survey, and 38% in the October 1991 survey, to
41% in this survey.


User ratings

We asked members to rate their network performance monitors
on a scale from one ('poor') to five ('excellent') using the
following twelve criteria:

o      Range of measurements
o      Historical data collection
o      Batch reporting
o      Threshold warnings
o      Problem/bottleneck analysis
o      Reporting of statistics
o      Session manager interface
o      Performance overhead
o      Reliability
o      Ease of use
o      Vendor support
o      Value for money.

In addition, members were asked to assess how accurately and
completely their network performance monitors measured the
following aspects of VTAM/network performance - again
using a scale from one ('poor') to five ('excellent'):

o      End-user response times
o      Host response times
o      Network response times
o      Line utilization
o      Buffer statistics
o      Channel I/O
o      Error rates
o      NCP cycle utilization
o      Virtual routes
o      Storage analysis.

Figure 9 shows the average ratings awarded by members to
their network performance monitors and Figure 10 shows the
ratings for the degree of accuracy and completeness with which
packages monitor various aspects of network performance.
Figures 9 and 10 are arranged with the package with the
highest aggregate score at the top. The sample sizes for the
ratings charts are as follows: 86 members rated NetSpy, 124
rated NetView PM, 20 rated Omegamon for VTAM, eight
rated The Monitor for VTAM, and 14 rated Vital Signs for VTAM.

On average, members awarded higher ratings for reliability
than for other criteria - 'all packages' were rated nearer 'good'
(4) than 'satisfactory' (3) in this category. Packages also
scored, on average, very well for the range of measurements
they collect and for their vendor support, again scoring nearer
'good' than 'satisfactory' for both criteria. The average ratings
for threshold warnings and performance overhead are only
marginally nearer 'satisfactory' than 'good', as are, to a lesser
extent, those for value for money and historical data collection.
The only criterion for which all packages were, on average,
rated less than 'satisfactory' is batch reporting.

The aspect of network performance that packages were
reckoned, on average, to have monitored most accurately and
completely is line utilization. Network response times, host
response times, error rates, and end-user response times were
also considered, on average, to be monitored well - all
packages were, on average, awarded ratings nearer 'good' than
'satisfactory' for each of the above criteria. Packages were
awarded average ratings of 'satisfactory' or better for
monitoring NCP cycle utilization, buffer statistics, virtual
routes, and channel I/O activity, and ratings somewhat below
'satisfactory' for storage analysis.

Figure 11 shows the average ratings awarded to packages
plotted consecutively on a single axis. Note that we've
included the average each package scored for the degree of
accuracy and completeness with which it monitors all the
chosen aspects of network performance as a single rating in
Figure 11 under the heading 'quality of measurements'.

The Monitor for VTAM achieves the highest overall ratings,
coming first for no fewer than nine of the twelve criteria (range
of measurements, threshold warnings, problem/bottleneck
analysis, reporting of statistics, session manager interface,
reliability, ease of use, vendor support, and value for money)
and also achieves the highest average ratings for its overall
measurement of network performance. The Monitor comes
second for historical data collection and performance overhead,
and third for batch reporting. It is awarded average ratings of
'good' or better for nine criteria, average ratings nearer 'good'
than 'satisfactory' for another two criteria (historical data
collection and session manager interface), and only scores
below 'satisfactory' for one criterion (batch reporting - the
only criterion for which it scores below average). However, it
should be stressed that these ratings are based on only eight
responses.

NetSpy comes second overall, having come first for its batch
reporting (only NetSpy and Omegamon have average ratings of
'satisfactory' or better for batch reporting), second for three
criteria (range of measurements, session manager interface, and
reliability) and third for all the remaining criteria. NetSpy is the
only package to score above average for all twelve criteria, and
also has an above average rating for its overall measurement of
network performance. NetSpy's average rating for reliability is
slightly better than 'good', and for all the remaining criteria it
scores average ratings between 'satisfactory' and 'good'.

Vital Signs for VTAM is the third best rated package overall,
coming first for historical data collection and performance
overhead, and second for another four criteria (reporting of
statistics, ease of use, vendor support, and value for money).
Vital Signs also comes second for its overall measurement of
network performance, and scores above average for all but
three of our criteria (batch reporting, threshold warnings, and
session manager interface).

Omegamon for VTAM comes fourth overall; its best ratings are
for batch reporting, threshold warnings, and
problem/bottleneck analysis - coming second for each.
NetView Performance Monitor fares rather badly in the ratings,
coming last for ten of the twelve criteria and second from last
for the remaining two. The package is rated below average for
all criteria.


Best and worst features

We asked members what they consider the best and worst
features of their packages. In this section we summarize the
comments members made. Note that some of the comments
quoted below are paraphrased versions of what members wrote.

We received 63 comments from members expressing their
opinion of NetSpy's best feature, and 52 from members
expressing their opinion of the package's worst feature.
Nineteen members considered NetSpy easy to use, another
three found the package easy to install, one found it easy to
customize, and one found it easy to implement. A number of
members listed one or more of the measurements taken by
NetSpy as its best feature - they include four who singled out
line utilization, two who singled out end-to-end response time,
three who singled out end-user response time, one who singled
out network response time, and one who singled out buffer
usage. Three members liked the on-line reports and three liked
the on-line alerts and threshold warnings. Three members
found NetSpy's performance overhead frugal, three found the
package flexible, two found it reliable, and two liked the way
the package reports statistics. Other comments about NetSpy
include: "ability to interact with TPX", "speed of information",
"easy to equate real terminal name to logical terminal", "feeds
MICS database, handles virtual/real terminal mapping", "hot-
key set-up", "output records are simple in format and the SAS
record definitions make it easy to port records to SAS", "trace
facilities", and "once installed, it works immediately". One
user simply thought there were "so many Ýgood features¨ to
choose from"!

NetSpy's user interface elicited critical comments from seven
members, including one who was critical of the "cumbersome
and confusing panels", one who complained that the "panels
don't retain values, which have to be re-entered", and another
who found the "presentation and user interface a little creaky
and old-fashioned". Seven members found using NetSpy
incurred too great a performance overhead, including two who
thought the package used too many CPU cycles, two who
complained of excessive use of DASD storage, and one who
thought the package used too much memory. Five members
were critical of NetSpy's batch reporting (one of them thought
the batch reporting "esoteric", one criticized the "inability to
exclude things from the batch reporting utility", and another
"especially" disliked the batch reporting of historical data).
Another two members were critical of the reporting facilities as
a whole while a third member bemoaned the lack of on-line
graphics. Five members found NetSpy difficult to use, three
thought the package needed a better on-line help facility, and
one disliked the documentation supplied with the package. A
number of the critical comments we received concerned aspects
of NetSpy's performance monitoring - they include: "cannot
monitor individual applications within a CICS region",
"definite response time measurement adversely affects Unisys
Ý!¨ clusters in the network", "does not provide breakdown of
network performance from host out", "as yet doesn't give exact
response time at the end-user level", "inability to capture user,
host, and network response times simultaneously", "lack of
channel information", "no LAN monitoring", "Ýpoor¨
monitoring of cross-domain applications", and "no response
times in X.25". Other critical comments about NetSpy include:
"Ýhave suffered¨ a couple of failures over two years", "Ýpoor¨
interfaces with NetView AS", "poor history of tuning
statistics", "SMF data for sessions running under session
manager are included in session manager statistics", "inflexible
real-time control", "Ýpoor¨ threshold warnings", "too many
parameters to worry about", "Ýpoor¨ tuning and
recommendations", "Ýpoor¨ vendor support", and "Ýit needs¨
more diagnostic capabilities".

67 of the comments we received about NetView PM concerned
its best feature and 69 concerned its worst feature. Eight
members liked NetView PM's interface with the base package,
NetView (two of them emphasized how this aids in automating
the network - another two members simply thought NetView's
best feature was its network automation). Two members liked
NetView PM's interface to VTAM, one member liked the
interface with NCP/NPSI, and one member liked the interface
with "other IBM communications programs". The way
NetView PM's monitors various aspects of network
performance was praised by a number of members, including
its measurement of line utilization (listed by four members, one
of whom noted how the package monitors "terminal by
terminal statistics on a line"), end-to-end response time (two
members), end-user response time (two members), and NCP
cycle utilization (one member). Two members liked the way
the package provides session statistics, three were impressed
with the problem and bottleneck analysis, two liked the
threshold warnings provided (one singled out "NetView's
generic ALERT function"), two praised the range of
measurements taken by NetView, one liked the "ability to get
into LANs", and one liked the "Token Ring information".
NetView's performance overhead was considered frugal by
three members, one of whom cited the package's low
consumption of CPU cycles. Three members found NetView
reliable, two liked the vendor support, two liked the user
interface, two liked the package's graphics, two liked the on-
line statistics, and one thought the package secure. Other
comments in praise of NetView include: "ability to control
clusters", "ease and speed of switching applications", "easy to
obtain", "flexible", "network review data", "Ýthe package¨
outputs to SLR for consolidated reports", "remote testing and
configuration facilities", "stability", and "up to date with IBM
technology".

Fourteen members found NetView PM "difficult to use",
including one who wrote that the package "requires a highly
technical individual to use"; another four members disliked the
user interface, and one member found the package's
presentation poor. Nine members complained of the lack of
adequate batch reporting, eight members found the package's
performance overhead excessive (one singled out the VTAM
performance, another the large memory requirement), four
members found the package "too difficult to customize" or
implement, two members complained of the lack of "tags" or
interfaces to third-party packages, and two members
complained that "adding an application often doesn't work
easily". Two members questioned NetView PM's reliability
(one described it as "bug-ridden", the other noted its ability to
"bring down VTAM"), one member complained of the
documentation, one complained of "unannounced and
undocumented record format changes", and five members
found the package lacked the facilities to monitor response
times adequately (one singled out end-to-end response times,
two singled out end-user response times, and the others weren't
specific). Two members found the package lacked LAN
monitoring (one of them singled out the monitoring of Token
Ring LANs - contradicting another member's opinion of the
package's best feature), two members found the facilities for
data gathering "too complex", and two members found
NetView PM lacked facilities to monitor X.25 networks. Other
critical comments about NetView Performance Monitor
include: "insufficient detail in diagnostic capability", "limited
analysis", "low functionality", "no user reporting", "runs in
CICS regions", and "screen design".

Eleven members wrote about Omegamon's best feature and
nine wrote about its worst feature. Three members found the
package easy to use, two liked its warning messages, one liked
its "interface with Candle's status monitor", one liked the
interface with NetView, one member noted the package's
excellent "monitoring of TNSTATS, buffer pool, and control
block queues", another was pleased with the way it handled
"response time problem determination", and two members
found the package "flexible". Other comments praising
Omegamon are: "Ýit's a¨ Candle product", "spool analysis",
and "task monitoring (kill etc)".

Four of the negative comments about Omegamon centre around
the package's ease of use - one notes the "unfriendly screens",
while another accuses the package of being "very awkward and
unforgiving". Two members found running Omegamon
incurred a "high cost", one disliked the "response time
methodology", one thought the package was "missing some
desirable messages", and another noted the lack of an
"interface to either AOC or ANO from IBM".

Four members wrote positive comments about The Monitor for
VTAM and five wrote negative ones. The positive comments
were: "cost", "ease of use", "easily customized", and "value
for money". The negative comments were: "Ýpoor¨ graphics",
"Ýlack of¨ integration with other Monitors from Landmark",
"no NetView interface", "not user friendly - no dynamic re-
configuration", and "Ýpoor¨ resource utilization".

Thirteen members wrote to praise Vital Signs for VTAM and
eleven members wrote comments critical of it. Four members
found Vital Signs easy to use, one commented that the package
"is easier to install than it used to be", another liked the
package's "SAA/CUA display". Three members were pleased
with the way the package measures end-user response times,
one member liked Vital Signs' "comprehensive statistics
collection", one was pleased with the collection of "network
and buffer statistics", and another noted that the package
"provides information not available elsewhere, thus helping to
spot trouble areas".

Nine members were critical of Vital Signs' reporting facilities -
four singled out the batch reports, including one who was
concerned that "selective batch reporting is not available -
would like time window option", and five members were
critical of its reporting in general (one wrote that "management
reporting" was so poor, "we've had to write our own reports",
one complained of "reporting and regeneration to match a new
NCP", and another noted that, while the reporting facilities
were poor, "they've promised to improve this in the next
release"). Other critical comments about Vital Signs were:
"difficulty in reviewing historical data", "high overhead", and
"session analysis".



